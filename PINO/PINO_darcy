import operator
import time
from functools import partial
from functools import reduce
from timeit import default_timer

import matplotlib.pyplot as plt
import numpy as np
import torch
import os
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.parameter import Parameter
from utilize.process_data import MatLoader

from train_utils.adam import Adam
from visual_data import *


################################################################
# fourier layer
################################################################
class SpectralConv2d(nn.Module):     #Fourier layer 的上支计算
    '''
    2维谱卷积
    Modified Zongyi Li's SpectralConv2d PyTorch 1.6 code
    using only real weights
    https://github.com/zongyi-li/fourier_neural_operator/blob/master/fourier_2d.py
    '''

    def __init__(self, in_dim,
                 out_dim,
                 modes: tuple,  # number of fourier modes
                 dropout=0.1,
                 norm='ortho',
                 activation='gelu',
                 return_freq=False):
        super(SpectralConv2d, self).__init__()

        """
        2D Fourier layer. It does FFT, linear transform, and Inverse FFT.    
        """

        self.in_dim = in_dim
        self.out_dim = out_dim
        if isinstance(modes, int):
            self.modes1 = modes  # Number of Fourier modes to multiply, at most floor(N/2) + 1  #输入模态
            self.modes2 = modes#输出模态
        else:
            self.modes1 = modes[0]  # Number of Fourier modes to multiply, at most floor(N/2) + 1
            self.modes2 = modes[1]

        self.norm = norm
        self.dropout = nn.Dropout(dropout)
        self.activation = activation_dict[activation]
        self.return_freq = return_freq
        self.linear = nn.Conv2d(self.in_dim, self.out_dim, 1)  # for residual

        self.scale = (1 / (in_dim * out_dim))
        self.weights1 = nn.Parameter(   #权重一     类型为复数类型dtype=torch.cfloat
            self.scale * torch.rand(in_dim, out_dim, self.modes1, self.modes2, dtype=torch.cfloat))
        self.weights2 = nn.Parameter(   #权重二
            self.scale * torch.rand(in_dim, out_dim, self.modes1, self.modes2, dtype=torch.cfloat))

    # Complex multiplication
    def compl_mul2d(self, input, weights):
        # (batch, in_channel, x,y ), (in_channel, out_channel, x,y) -> (batch, out_channel, x,y)
        return torch.einsum("bixy,ioxy->boxy", input, weights)   #爱因斯坦分析 ：张量

    def forward(self, x):
        """
        forward computation
        """
        batch_size = x.shape[0]
        # Compute Fourier coeffcients up to factor of e^(- something constant)
        res = self.linear(x)
        x = self.dropout(x)
        x_ft = torch.fft.rfft2(x, norm=self.norm)

        # Multiply relevant Fourier modes
        out_ft = torch.zeros(batch_size, self.out_dim, x.size(-2), x.size(-1) // 2 + 1, dtype=torch.cfloat,
                             device=x.device)   #R部分
        out_ft[:, :, :self.modes1, :self.modes2] = \
            self.compl_mul2d(x_ft[:, :, :self.modes1, :self.modes2], self.weights1)
        out_ft[:, :, -self.modes1:, :self.modes2] = \
            self.compl_mul2d(x_ft[:, :, -self.modes1:, :self.modes2], self.weights2)

        # Return to physical space   傅里叶逆变换
        x = torch.fft.irfft2(out_ft, s=(x.size(-2), x.size(-1)), norm=self.norm)
        x = self.activation(x + res)

        if self.return_freq:
            return x, out_ft
        else:
            return x



class FNO2d(nn.Module):
    """
        2维FNO网络
    """

    def __init__(self, in_dim, out_dim, modes=(8, 8), width=32, depth=4, hidden=128, steps=1, padding=2,
                 activation='gelu', dropout=0.0):
        super(FNO2d, self).__init__()

        self.in_dim = in_dim
        self.out_dim = out_dim
        self.modes = modes
        self.width = width
        self.depth = depth
        self.hidden = hidden
        self.steps = steps
        self.padding = padding  # pad the domain if input is non-periodic
        self.activation = activation
        self.dropout = dropout
        self.fc0 = nn.Linear(steps * in_dim + 2, self.width)   #对通道数提升，从steps * in_dim加2：2是坐标加了x,y两个维度。提升到self.with（自己定义的）
        # input channel is 12: the solution of the first 10 timesteps + 2 locations (u(1, x, y), ..., u(10, x, y),  x, y, t)

        self.convs = nn.ModuleList()
        for i in range(self.depth):
            self.convs.append(
                SpectralConv2d(self.width, self.width, self.modes, activation=self.activation, dropout=self.dropout,
                               norm=None))   # SpectralConv2d上支

        self.fc1 = nn.Linear(self.width, self.hidden)
        self.fc2 = nn.Linear(self.hidden, out_dim)

    def feature_transform(self, x):    #归一化坐标，加了两个维度
        """
        Args:
            x: input coordinates
        Returns:
            res: input transform
        """
        shape = x.shape
        batchsize, size_x, size_y = shape[0], shape[1], shape[2]
        gridx = torch.linspace(0, 1, size_x, dtype=torch.float32)
        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])
        gridy = torch.linspace(0, 1, size_y, dtype=torch.float32)
        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])
        return torch.cat((gridx, gridy), dim=-1).to(x.device)

    def forward(self, x, grid=None):
        """
        forward computation
        """
        # x dim = [b, x1, x2, t*v]
        if grid is None:
            grid = self.feature_transform(x)
        x = torch.cat((x, grid), dim=-1)
        x = self.fc0(x)  #此时x变为三维，因为加了x,y方向的feature
        x = x.permute(0, 3, 1, 2)  #调整通道数  HWC-> CHW

        if self.padding != 0:
            x = F.pad(x, [0, self.padding, 0, self.padding])  # pad the domain if input is non-periodic

        for i in range(self.depth):
            x = self.convs[i](x)

        if self.padding != 0:
            x = x[..., :-self.padding, :-self.padding]
        x = x.permute(0, 2, 3, 1)  # pad the domain if input is non-periodic
        x = self.fc1(x)   #降通道，从width降到**例如128
        x = F.gelu(x)  #加了一个激活函数
        x = self.fc2(x)   #再降通道，从128降到1
        return x

# 使用FDM solve darcy中的拉普拉斯算子 (中心有限差分法)
def FDM_Darcy(u, a, D=1, f=1):  # u:fno  a:input
    batchsize = u.size(0)  # 张量u的批量大小
    size = u.size(1)  # 维度大小
    u = u.reshape(batchsize, size, size)  # （batchsize, 空间维度x, 空间维度y）
    a = a.reshape(batchsize, size, size)
    dx = D / (size - 1)  # 计算格点间的网格间距
    dy = dx

    # ux: (batch, size-2, size-2)   #去除了边界
    ux = (u[:, 2:, 1:-1] - u[:, :-2, 1:-1]) / (2 * dx)  # 中心有限差分，计算相邻格点的差分。（2*dx）表示x方向的有限差分步长。
    uy = (u[:, 1:-1, 2:] - u[:, 1:-1, :-2]) / (2 * dy)  # 注意切片方法：取每行的第二个元素到倒数第二个（不包括）--->为了避免处理数组的边界元素。

    ax = (a[:, 2:, 1:-1] - a[:, :-2, 1:-1]) / (2 * dx)
    ay = (a[:, 1:-1, 2:] - a[:, 1:-1, :-2]) / (2 * dy)
    uxx = (u[:, 2:, 1:-1] - 2 * u[:, 1:-1, 1:-1] + u[:, :-2, 1:-1]) / (dx ** 2)  # 拉普拉斯算子
    uyy = (u[:, 1:-1, 2:] - 2 * u[:, 1:-1, 1:-1] + u[:, 1:-1, :-2]) / (dy ** 2)

    a = a[:, 1:-1, 1:-1]  # 去除边界上的数据
    u = u[:, 1:-1, 1:-1]

    aux = a * ux
    auy = a * uy
    auxx = (aux[:, 2:, 1:-1] - aux[:, :-2, 1:-1]) / (2 * dx)
    auyy = (auy[:, 1:-1, 2:] - auy[:, 1:-1, :-2]) / (2 * dy)
    eqs = - (auxx + auyy)   #计算拉普拉斯算子

    return eqs

coords = coords.cuda()
mollifier = torch.sin(np.pi*coords_x) * torch.sin(np.pi*coords_y) * 0.001    #sin(pai*x)sin(pai*y)   零边界条件

#PINO: 提供了方程损失、边界条件损失  FNO：提供数据损失
def PINO_loss(u, a):  # u是fno的预测值，a是input即数据真实值
    batchsize = u.size(0)
    x, y = u.size(1)
    u = u.reshape(batchsize, x, y)
    a = a.reshape(batchsize, x, y)
    lploss = FieldsLpLoss(size_average=True)

    #构建上、下、左、右边界索引
    index_x = torch.cat([torch.tensor(range(0, x)), (x - 1) * torch.ones(x), torch.tensor(range(x - 1, 1, -1)),
                         torch.zeros(x)], dim=0).long()  # （边界左侧；边界上侧；边界右侧；边界下侧）
    index_y = torch.cat([(y - 1) * torch.ones(y), torch.tensor(range(y - 1, 1, -1)), torch.zeros(y),
                         torch.tensor(range(0, y))], dim=0).long()  # （边界上侧；边界右侧；边界下侧；边界左侧）

    boundary_u = u[:, index_x, index_y]  # 提取了u在边界上的值
    truth_u = torch.zeros(boundary_u.shape, device=u.device)  # 与u相同的全0数组-->达西流的边界条件（第一类边界条件）
    loss_bd = lploss.abs(boundary_u, truth_u)  #计算fno预测与真实值的绝对误差

    eqs = FDM_Darcy(u, a)  # 计算了u 和 a的拉普拉斯算子
    f = torch.ones(eqs.shape, device=u.device)
    loss_f = lploss.rel(eqs, f, std=False)  

    return loss_f, loss_bd  # equation loss; boundary loss.


def train(dataloader, Net_model, FieldsLpLoss, optimizer, scheduler):
    train_pino = 0
    train_fno = 0
    train_loss = 0
    # for iteration in range(epochs):
    #     Net_model.train()
    #     t1 = default_timer()

    # 遍历训练集
    for xx, yy in dataloader:
        xx, yy = xx.cuda(), yy.cuda()  # x: input of training set; y: output of the truth

        out = Net_model(xx)  # x: the input of model
        out = out * mollifier
        # 计算PINN loss
        loss_data = FieldsLpLoss(out, yy)  # compute the output of FNO predicted and true data
        loss_f, loss_bd = PINO_loss(out, xx)
        pino_loss = loss_f
        loss_batch = 10 * loss_f + 1 * loss_bd + 10 * loss_data
  
        optimizer.zero_grad()
        loss_batch.backward()

        optimizer.step()
        train_fno += loss_data.item()
        train_pino += pino_loss.item()
        train_loss += torch.tensor([loss_bd, loss_f])

        scheduler.step()

    return train_loss, train_fno, train_pino


def valid(dataloader, Net_model, FieldsLpLoss):
    valid_pino = 0
    valid_fno = 0
    valid_loss = 0
    # for iteration in range(epochs):
    #     Net_model.valid()
    #     t1 = default_timer()

    # 遍历训练集
    for xx, yy in dataloader:
        xx, yy = xx.cuda(), yy.cuda()  # x: input of training set; y: output of the truth
        out = Net_model(xx)  # x: the input of model
        out = out * mollifier
        # 计算PINN loss
        loss_data = FieldsLpLoss(out, yy)  # compute the output of FNO predicted and true data
        loss_f, loss_bd = PINO_loss(out, xx)
        pino_loss = loss_f
        loss_batch = 10 * loss_f + 1 * loss_bd + 10 * loss_data

        valid_fno += loss_data.item()
        valid_pino += pino_loss.item()
        valid_loss += torch.tensor([loss_bd, loss_f])

    return valid_fno, valid_pino, valid_loss


def inference(dataloader, Net_model):
    with torch.no_grad():
        xx, yy = next(iter(dataloader))
        xx = xx.cuda()
        pred = Net_model(xx)

    return xx.cpu().numpy(), xx.cpu().numpy(), yy.numpy(), pred.cpu().numpy()


if __name__ == "__main__":

    name = 'PINO'
    work_path = os.path.join('work', name)
    isCreated = os.path.exists(work_path)
    if not isCreated:
        os.makedirs(work_path)

    sys.stdout = TextLogger(filename=os.path.join(work_path, 'train.log'))

    # sys.stdout.info("Model Name: {:s}, Computing Device: {:s}".format(name, str(Device)))

    # 将控制台的结果输出到log文件
    sys.stdout = TextLogger(os.path.join(work_path, 'train.log'), sys.stdout)

    ################################################################
    # config
    ################################################################

    ntrain = 600
    nvalid = 100

    batch_size = 32
    learning_rate = 0.001

    epochs = 500
    scheduler_step = [200,300,400]
    gamma = 0.5

    modes = 12
    width = 32

    r = 1
    h = int(((128 - 1) / r) + 1)  # 128
    s = h

    print(h)

    ################################################################
    # load data
    ################################################################

    # nu   (10000,128,128) ; tensor(10000,1,128,128); x(128,)
    file_path = os.path.join('data','2D_DarcyFlow_beta1.0_Train.hdf5')
    reader = MatLoader(file_path)
    fields = reader.read_field('tensor').permute(3, 0, 1, 2)  # (128,128,1,10000)
    coords_x = reader.read_field('x-coordinate')  # (128,)
    coords_y = reader.read_field('y-coordinate')
    nu = reader.read_field('nu')
    nu_tile = torch.tile(nu[:, :, None, :], (1, 1, 1, 1)).permute(3, 0, 1, 2)  #(10000,128,128,1)

    # input = torch.concat((nu_tile, coords_title), dim=-1)
    input = nu_tile
    output = fields

    train_x = input[:ntrain, ::r, ::r][:, :s, :s]
    train_y = output[:ntrain, ::r, ::r][:, :s, :s]
    valid_x = input[ntrain:ntrain + nvalid, ::r, ::r][:, :s, :s]
    valid_y = output[ntrain:ntrain + nvalid, ::r, ::r][:, :s, :s]

    train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(train_x, train_y), batch_size=batch_size,
                                               shuffle=True, drop_last=True)
    valid_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(valid_x, valid_y), batch_size=batch_size,
                                               shuffle=False, drop_last=True)

    Net_model = FNO2d(in_dim=train_x.shape[-1], out_dim=train_y.shape[-1], modes=(32, 8), width=32, depth=4).cuda()
    optimizer = torch.optim.Adam(Net_model.parameters(), lr=learning_rate, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, scheduler_step=scheduler_step, gamma=gamma)

    FieldsLpLoss = FieldsLpLoss(size_average=False)

    Visual = MatplotlibVision(work_path, input_name=('x', 'y'), field_name=('u'))
    star_time = time.time()

    log_loss = [[], []]

    for epoch in range(epochs):
        if epoch > 0 and epoch % 1 == 0:
            train_coord1, train_grid, train_true, train_pred = inference(train_loader, Net_model)
            valid_coord1, valid_grid, valid_true, valid_pred = inference(valid_loader, Net_model)

            print('coord, pred ,true',train_coord1.shape,train_pred.shape,train_true.shape)  #(1,128,128,1)

            torch.save({'log_loss': log_loss, 'net_model': Net_model.state_dict(), 'optimizer': optimizer.state_dict()},
                       os.path.join(work_path, 'latest_model.pth'))

            fig, axs = plt.subplots(1, 3, figsize=(18, 10), num=1)
            axs_flat = axs.flatten()
            for ax in axs_flat:
                ax.axis('off')
                ax.set_frame_on(False)
            Visual.plot_fields_ms(fig, axs, train_true[:,:,1], train_pred[:,:,1], train_coord1[:,:,1])
            fig.savefig(os.path.join(work_path, 'train_solution_' + str(0) + '_local_01.jpg'), dpi=600,
                        bbox_inches='tight')
            plt.close(fig)

#############
# NOTE:
# 1、FNO的data loss加入总loss，权重分配------>
# 2、边界条件需要加上，当2的coords解决后
# 3、数据预处理coords—x，y-------可视化



