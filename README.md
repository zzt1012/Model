# åŸºäºæ·±åº¦å­¦ä¹ çš„çº³ç±³æµä½“ç‰©ç†åœºä¸æ€§èƒ½é¢„æµ‹ç ”ç©¶-----æŠ€æœ¯æ–‡æ¡£



- è¿‘å¹´æ¥ï¼Œéšç€å·¥ä¸šç”Ÿäº§ç­‰é¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼Œå¤§åŠŸç‡è®¾å¤‡åœ¨è¿è¡Œè¿‡ç¨‹ä¸­ä¼šäº§ç”Ÿå¤§é‡çš„çƒ­é‡ï¼Œå¯¼è‡´è®¾å¤‡æ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜ä¼ çƒ­èƒ½åŠ›ï¼Œæ”¹å–„ä¼ çƒ­è¿‡ç¨‹ï¼Œé‡‡ç”¨çº³ç±³æµä½“æŠ€æœ¯ï¼Œåˆ©ç”¨æºæ‚çº³ç±³é¢—ç²’æ¥æé«˜æµä½“çš„å¯¼çƒ­æ€§èƒ½ï¼Œä»è€Œå¢å¼ºå†·å´æ•ˆæœã€‚å®éªŒæµ‹é‡å¯¹äºç†è§£çº³ç±³æµä½“çš„æµåŠ¨å’Œä¼ çƒ­ç‰¹æ€§èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œä½†ç”±äºæµ‹é‡æŠ€æœ¯çš„åŸå› ï¼Œç¼ºä¹è¯¦ç»†çš„å…¨å±€ç»†èŠ‚ï¼Œè¿™å¯¹è¯¥é¢†åŸŸçš„å®éªŒç ”ç©¶é€ æˆäº†é‡å¤§é™åˆ¶ã€‚å› æ­¤ï¼Œå—åˆ°è®¡ç®—æœºè§†è§‰ä¸­çš„å›¾åƒå›å½’æŠ€æœ¯çš„å¯å‘ï¼Œå„ç§æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å·¥ç¨‹ä¸­å¹¿æ³›åº”ç”¨äºç‰©ç†é¢†åŸŸçš„å»ºæ¨¡ã€‚é€šè¿‡å°†å®Œæ•´çš„ç‰©ç†åœºè§†ä¸ºåœ¨ç©ºé—´åŸŸä¸­å®šä¹‰çš„å›¾åƒï¼Œæˆ‘ä»¬å¯ä»¥å°†ä¸åŒçš„ç‰©ç†é‡æ¦‚å¿µåŒ–ä¸ºè¯¥å›¾åƒçš„ä¸åŒRGBé€šé“ã€‚è¿™ç§ç±»æ¯”ä½¿æˆ‘ä»¬èƒ½å¤Ÿåˆ©ç”¨å›¾åƒå¤„ç†ä¸­ä½¿ç”¨çš„åŸåˆ™å’Œæ–¹æ³•æ¥æœ‰æ•ˆåœ°åˆ†æå’Œå»ºæ¨¡ç‰©ç†æ•°æ®ã€‚

- æœ¬æ–‡åŸºäºäº”ç§ç¥ç»ç®—å­ç½‘ç»œï¼šFNOã€U-Netã€FNNã€DeepONetå’ŒTransformerï¼Œå¯¹water-Al2O3çº³ç±³æµä½“åœ¨å¾®é€šé“å†…çš„æµåŠ¨è¿›è¡Œç‰©ç†åœºé¢„æµ‹ï¼Œå¹¶ä»é¢„æµ‹åœºä¸­æå–äº†è¡¨å¾æµåŠ¨æ¢çƒ­æ€§èƒ½çš„å‚æ•°Nusseltæ•°å’ŒFanningæ‘©æ“¦å› å­ã€‚å°†äºŒç»´ç‰©ç†åœºæ±‚è§£è¿‡ç¨‹çœ‹ä½œç»™å®šè®¾è®¡å˜é‡ä¸‹çš„ç‰©ç†åœºå›å½’ä»»åŠ¡ï¼šä»¥å¾®é€šé“çš„8ä¸ªè®¾è®¡å˜é‡åŠç©ºé—´åæ ‡ä¸ºè¾“å…¥ï¼Œè¾“å‡º4ä¸ªç‰©ç†åœºï¼ˆå‹åŠ›ã€æ¸©åº¦ã€é€Ÿåº¦åœºï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å…³æ³¨äº†å·¥ä¸šä¸­ä»ç‰©ç†åœºæå–çš„å®é™…æ€§èƒ½å‚æ•°çš„é¢„æµ‹ç²¾åº¦ï¼Œé‡‡ç”¨ç§¯åˆ†æ±‚è§£çš„æ–¹å¼ï¼Œå®ç°å¯¹Nusseltæ•°å’ŒFanningæ‘©æ“¦å› å­çš„é¢„æµ‹ã€‚
- æœ¬æ¬¡å¯¹äº”ç§ç¥ç»ç®—å­ç½‘ç»œçš„å®ç°å‡åŸºäºPaddleï¼Œä¸”æ‰€æœ‰æ¨¡å‹è®­ç»ƒåœ¨ AMD EPYC 7642  CPU å’Œ Nvidia A40 GPU å·¥ä½œç«™è¿›è¡Œã€‚é€šè¿‡ç®€å•å¯¹æ¯”äº†äº”ç§ç½‘ç»œé¢„æµ‹çš„ç‰©ç†åœºè¯¯å·®ï¼Œç»“æœè¡¨æ˜FNOå’ŒTransformeré¢„æµ‹ç²¾åº¦ç›¸å¯¹è¾ƒé«˜ï¼ŒU-Netå’ŒDeepONetçš„é¢„æµ‹æ•ˆæœæœ€å·®ã€‚



## 1. ä»£ç è¯´æ˜



ğŸ“‚ Deep-Flow-Prediction-Paddle

|_ğŸ“ data                                                             #éƒ¨åˆ†æ•°æ®é›†

â€‹     |_ ğŸ“„ dim_pro8_single_try.mat

|_ğŸ“ config

â€‹     |_ğŸ“„ CNN.yaml                                             # CNNçš„ç½‘ç»œå‚æ•°è®¾ç½®

â€‹     |_ğŸ“„ DNO.yaml                                            # DNOçš„ç½‘ç»œå‚æ•°è®¾ç½®

â€‹     |_ğŸ“„ FNO.yaml                                             # FNOçš„ç½‘ç»œå‚æ•°è®¾ç½®

â€‹     |_ğŸ“„ MLP.yaml                                             # FNNçš„ç½‘ç»œå‚æ•°è®¾ç½®

â€‹     |_ğŸ“„ TNO.yaml                                            # TNOçš„ç½‘ç»œå‚æ•°è®¾ç½®

|_ğŸ“ src

â€‹    |_ğŸ“„ process_data                                       # è¯»å–matlabæ ¼å¼æ•°æ®ï¼›æ•°æ®å½’ä¸€åŒ–ï¼›åˆ’åˆ†æ•°æ®é›†åŠæ•°æ®é‡‡æ ·

â€‹    |_ğŸ“„ CNN_model.py                                   # äºŒç»´U-Net model paddleä»£ç 

â€‹    |_ğŸ“„ DON_model.py                                  # äºŒç»´DeepONet ä»¥åŠ FNN model paddleä»£ç 

â€‹    |_ğŸ“„ FNO_model.py                                   # äºŒç»´Fourier Neural Operator paddleä»£ç 

â€‹    |_ğŸ“„ TNO_model.py                                   # äºŒç»´Transformer paddleä»£ç , æ”¯æŒå¤šç§attentionæœºåˆ¶ä»¥åŠä¸¤ç§Regressor

â€‹    |_ğŸ“„ neural_model.py                               # ç½‘ç»œçš„è®­ç»ƒåŠéªŒè¯ï¼›æ€§èƒ½å‚æ•°çš„ç§¯åˆ†æ±‚è§£ï¼›å¯è§†åŒ–æŸå¤±å‡½æ•°ã€ç‰©ç†åœºã€æ€§èƒ½å‚æ•°

â€‹    |_ğŸ“„ process_data.py                                # æ•°æ®è¯»å–ï¼›æ•°æ®å½’ä¸€åŒ–ï¼›æ•°æ®é›†åˆ’åˆ†ï¼›

â€‹    |_ğŸ“„ utilize.py                                             # æ¿€æ´»å‡½æ•°ï¼›æŸå¤±å‡½æ•°ï¼›åˆå§‹åŒ–æƒé‡ï¼›è®°å½•è®­ç»ƒä¿¡æ¯

â€‹    |_ğŸ“„ visual_data.py                                    # å¯è§†åŒ–ä»£ç 

|_ğŸ“ work                                                       # è®­ç»ƒè¿‡ç¨‹ã€éªŒè¯ç»“æœã€æµ‹è¯•ç»“æœï¼Œç»Ÿè®¡ç»“æœæ–‡ä»¶ä¿å­˜

â€‹    |_ğŸ“ DON                                                   # DeepONetè®­ç»ƒç»“æœä¿å­˜

â€‹         |_ğŸ“ 2023-10-07-10-43                       # ä»¥æ—¶é—´æˆ³å‘½åå¯¹ç»“æœä¿å­˜

â€‹                |_ğŸ“ infer                                       # æ¨¡å‹è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•ç»“æœä¿å­˜

â€‹                |_ğŸ“ train                                      # è®­ç»ƒç»“æœä¿å­˜

â€‹                |_ğŸ“ valid                                      # éªŒè¯ç»“æœä¿å­˜

â€‹                |_ğŸ“„ last_model.pdparams        # ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶

â€‹                |_ğŸ“„ loghistory.pkl                       # ä¿å­˜çš„epochã€è®­ç»ƒå’Œé¢„æµ‹æ—¶é—´ã€ç‰©ç†åœºåŠæ€§èƒ½å‚æ•°æŸå¤±æ–‡ä»¶

â€‹    |_ğŸ“ FNO                                                   # FNOè®­ç»ƒç»“æœä¿å­˜

â€‹    |_ğŸ“ CNN                                                  # U-Netè®­ç»ƒç»“æœä¿å­˜

â€‹    |_ğŸ“ TNO                                                  # Transformerè®­ç»ƒç»“æœä¿å­˜

â€‹    |_ğŸ“ FNN                                                  # FNNè®­ç»ƒç»“æœä¿å­˜

|_ğŸ“„ run_infer.py                                         # æµ‹è¯•è¿‡ç¨‹

|_ğŸ“„ run_main.py                                       # è®­ç»ƒè¿‡ç¨‹ã€éªŒè¯è¿‡ç¨‹



### 1.1 è®­ç»ƒå‚æ•°è®¾ç½®

```python
basic_config:
  root_path: './'                        #è¯»å–æ•°æ®è·¯å¾„
  data_name: 'dim_pro8_single_all.mat'   #æ•°æ®é›†åç§°
  training_size: 0.8                     #è®­ç»ƒé›†å æ ·æœ¬çš„æ¯”ä¾‹
  batch_size: 32                         #æ¯æ¬¡ä¼ é€’ç»™ç½‘ç»œç”¨æ¥è®­ç»ƒçš„æ ·æœ¬ä¸ªæ•°
  total_epoch: 500                       #è®­ç»ƒæ€»æ­¥é•¿
  loss_name: 'mse'                       #æŸå¤±å‡½æ•°ç±»å‹
  learning_rate: 1.e-4                   #å­¦ä¹ ç‡
  weight_decay: 1.e-12                   #æƒé‡è¡°å‡
  learning_beta:                         #æ§åˆ¶æ¢¯åº¦ä¿¡æ¯çš„å‚æ•°
    - 0.7
    - 0.9
  learning_milestones:                   #å­¦ä¹ ç‡ä¸‹é™èŠ‚ç‚¹
    - 300
    - 400
    - 500
  learning_gamma: 0.1                    #å­¦ä¹ ç‡ä¸‹é™å€æ•°
```

### 1.2 å¿«é€Ÿè¿è¡Œ

- configæ–‡ä»¶ä¸‹çš„äº”ä¸ªyamlè„šæœ¬åˆ†åˆ«ä¸ºäº”ç§ç¥ç»ç®—å­ç½‘ç»œçš„å‚æ•°é…ç½®ã€‚å…¶ä¸­ï¼Œâ€œbasic_configâ€ä¸ºè®­ç»ƒå‚æ•°çš„è®¾ç½®ï¼Œâ€network_configâ€œæ˜¯æ¨¡å‹å‚æ•°çš„è®¾ç½®ï¼Œtraining_sizeä¸ºè®­ç»ƒé›†å æ€»æ ·æœ¬çš„æ¯”ä¾‹ï¼Œbatch_sizeä¸ºå•æ¬¡ä¼ é€’ç»™ç¨‹åºç”¨ä»¥è®­ç»ƒçš„æ•°æ® (æ ·æœ¬) ä¸ªæ•°ï¼Œtotal_epochä¸ºè®­ç»ƒçš„æ€»æ­¥é•¿ï¼Œloss_nameä¸ºç‰©ç†åœºæˆ–æ€§èƒ½å‚æ•°çš„æ€»æŸå¤±çš„ç±»å‹ï¼›learning_rateã€weight_decayã€learning_betaã€learning_milestones å’Œlearning_gammaåˆ†åˆ«ä¸ºå­¦ä¹ ç‡ï¼Œæƒé‡è¡°å‡ï¼ŒAdamä¸­æ§åˆ¶æ¢¯åº¦ä¿¡æ¯çš„è¶…å‚æ•°ï¼Œå­¦ä¹ ç‡ä¸‹é™çš„èŠ‚ç‚¹ï¼Œå­¦ä¹ ç‡æ¯æ¬¡ä¸‹é™çš„å€æ•°ã€‚â€name_modelâ€œä¸ºä¸åŒç®—å­ç½‘ç»œçš„å…·ä½“å‚æ•°è®¾ç½®ã€‚

- è¿è¡Œrun_main.pyï¼Œå¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒå’ŒéªŒè¯ã€‚é¦–å…ˆï¼Œç¡®å®šæƒ³è¦è®­ç»ƒçš„æ¨¡å‹ï¼Œåœ¨åŸå§‹é…ç½®æ–‡ä»¶pathå¤„ï¼Œè¾“å…¥ç›®æ ‡æ¨¡å‹çš„yamlæ–‡ä»¶ã€‚ï¼ˆconfigæ–‡ä»¶ä¸‹çš„æ‰€æœ‰æ¨¡å‹çš„yamlæ–‡ä»¶å†…çš„å‚æ•°è®¾ç½®ï¼Œå‡ä¸ºæˆ‘ä»¬è®­ç»ƒå¥½çš„å‚æ•°ï¼Œå¯ç›´æ¥è°ƒç”¨ï¼‰ã€‚å…¶æ¬¡ï¼Œåœ¨ç½‘ç»œå‚æ•°é…ç½®å¤„ï¼Œè¾“å…¥ä¸åŒçš„nameä¼šè°ƒç”¨ä¸åŒçš„ç½‘ç»œæ¨¡å‹ï¼Œåœ¨workæ–‡ä»¶ä¸‹ç”Ÿæˆå¯¹åº”åå­—çš„å­æ–‡ä»¶ï¼Œå­æ–‡ä»¶ä¸‹æœ‰ä»¥æ—¶é—´æˆ³å‘½åçš„è®­ç»ƒç»“æœã€‚åŒæ—¶è¾“å‡ºç‰©ç†çš„æ€»loss_trainå’Œloss_validï¼Œæ€§èƒ½å‚æ•°çš„æ€»æŸå¤±loss_target_trainå’Œloss_target_validï¼Œå››ä¸ªç‰©ç†åœºçš„å„è‡ªçš„æŸå¤±metric_trainå’Œmetric_validï¼Œä¸¤ä¸ªæ€§èƒ½å‚æ•°çš„å„è‡ªæŸå¤±metric_target_trainå’Œmetric_target_validã€‚è¿è¡Œåä¼šç”Ÿæˆ20ä¸ªcaseçš„ç‰©ç†åœºäº‘å›¾ã€æ€§èƒ½å‚æ•°å›å½’å›¾ï¼ŒåŠå„ä¸ªè®­ç»ƒè¿‡ç¨‹çš„æŸå¤±æ›²çº¿ã€‚
- è¿è¡Œrun_infer.pyï¼Œé¦–å…ˆä¿®æ”¹ç½‘ç»œåç§°ï¼Œè¾“å…¥å¯¹åº”æ¨¡å‹çš„yamlæ–‡ä»¶è·¯å¾„ã€‚å…¶æ¬¡ï¼ŒæŠŠrun_main.pyä¸­å·²ç»è®­ç»ƒå¥½å¹¶ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶ï¼ˆåœ¨workæ–‡ä»¶ä¸‹çš„å¯¹åº”æ¨¡å‹åç§°å’Œæ—¶é—´æˆ³ä¸‹çš„last_model.pdparamsï¼‰è·¯å¾„æ·»åŠ åˆ°load_pathä¸­ï¼ŒModuleé€šè¿‡åŠ è½½å…¶è·¯å¾„ï¼Œä¼ é€’ä¿å­˜çš„configå’Œnetwork_configï¼Œè°ƒç”¨BasicModuleã€‚é€šè¿‡è¾“å…¥æµ‹è¯•é›†ï¼ˆå³test_datasetä¸­mode=1ï¼‰å®ç°å¯¹æ¨¡å‹çš„å¿«é€Ÿé¢„æµ‹ã€‚
- è€ƒè™‘åˆ°è¿è¡Œæ—¶é—´è¿‡ä¹…ï¼Œä¸ºäº†å¿«é€ŸæŸ¥çœ‹è¿è¡Œç»“æœï¼Œå¯å…ˆè®¾ç½®basci_configä¸­total_epoch=10ï¼Œ print_freq=1, save_freq=2ï¼Œè·‘å‡ºä¸€ç»„æŸ¥çœ‹æ•ˆæœï¼Œç»“æœåœ¨workæ–‡ä»¶ä¸‹å¯¹åº”ç®—å­ç½‘ç»œåç§°çš„å­æ–‡ä»¶çš„æ—¶é—´æˆ³ä¸­ä¿å­˜ã€‚

### 1.3 ç¯å¢ƒä¾èµ–

>  numpy==1.26.0
>
>  cudnn==8.4.1.50
>
>  cudatoolkit==11.6.2
>
>  paddlepaddle-gpu develop 2.5.1.
>
>  pyyaml==6.0
>
>  scipy==1.11.3
>
>  scikit-learning==1.3.0
>
>  seaborn==0.12.2
>
>  matplotlib==3.7.2



## 2. åŸå§‹æ•°æ®é›†

â€‹        æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨å•†ä¸šè½¯ä»¶ICEM CFDå’ŒFLUENTå¯¹ç½‘æ ¼è¿›è¡Œåˆ’åˆ†ä»¥åŠæ•°å€¼è®¡ç®—ï¼ŒåŸºäºæœ‰é™ä½“ç§¯æ³•ï¼Œå¯¹æ§åˆ¶æ–¹ç¨‹è¿›è¡Œæ—¶å‡åŒ–å¤„ç†ï¼Œæ‰€æœ‰æ–¹ç¨‹å‡é‡‡ç”¨SIMPLE ç®—æ³•ç»“åˆäºŒé˜¶è¿é£ç¦»æ•£åŒ–æ–¹æ¡ˆæ±‚è§£ã€‚å½“æ‰€æœ‰æ§åˆ¶æ–¹ç¨‹çš„æ®‹å·®å°äº 10-6ï¼Œä¸”ç›¸é‚»è¿­ä»£ä¹‹é—´å¹³å‡å£æ¸©å’Œå‹é™çš„æ®‹å·®å°äº 0.1%æ—¶ï¼Œè®¤ä¸ºæ•°å€¼æ¨¡å‹æ”¶æ•›ã€‚

 		æœ¬æ–‡ï¼Œä»¥Al2O3 çº³ç±³æµä½“åœ¨å¸¦æœ‰å‡¹æ§½çš„å¾®é€šé“ä¸­æµåŠ¨çš„äºŒç»´æµåŠ¨æ¢çƒ­é—®é¢˜ä½œä¸ºç ”ç©¶å¯¹è±¡ï¼ŒéªŒè¯ç‰©ç†åœºé¢„æµ‹ä»¥åŠæµåŠ¨æ¢çƒ­æ€§èƒ½çš„è¯†åˆ«æ–¹æ³•ã€‚ é‰´äºå…¶å‡ ä½•å½¢çŠ¶ï¼Œå¯ä»¥åœ¨æ²¿ z æ–¹å‘çš„ä»»ä½•ä½ç½®è·å¾— x-y å¹³é¢æ¨¡å‹ï¼Œå› æ­¤æˆ‘ä»¬å°†ä¸‰ç»´æ¨¡å‹ç®€åŒ–ä¸ºäºŒç»´æ¨¡å‹ï¼Œä¸”ç¤ºæ„å›¾å¦‚ä¸‹æ‰€ç¤ºã€‚



<img src="C:\Users\zt\Desktop\or2\é€šé“.jpg" style="zoom:50%;" />

### 1.1 æ•°æ®æè¿°

â€‹		æœ¬æ–‡æ‰€æœ‰çš„çŠ¶æ€å‚æ•°å‡å¯ä»¥ä»æ•°å€¼åˆ†æç»“æœ [1-2] ä¸­è·å–ï¼Œç‰©ç†åœºæ•°æ®æ˜¯å®šä¹‰åœ¨ç»“æ„åŒ–ç½‘æ ¼çš„æ¯ä¸ªèŠ‚ç‚¹ä¸Šã€‚æˆ‘ä»¬é‡‡ç”¨Latinè¶…ç«‹æ–¹é‡‡æ ·æ–¹æ³•é‡‡é›†äº†6773ä¸ªæ ·æœ¬ï¼Œå¹¶åˆ’åˆ†ä¸ºè®­ç»ƒé›†ï¼ˆ80%ï¼Œ5418ä¸ªæ ·æœ¬ï¼‰å’ŒéªŒè¯é›†ï¼ˆ10%ï¼Œ677ä¸ªæ ·æœ¬ä½œä¸ºéªŒè¯é›†å¯¹æ¨¡å‹è¿›è¡Œè°ƒä¼˜ï¼‰ã€æµ‹è¯•é›†ï¼ˆ10%ï¼Œ677 ä¸ªæ ·æœ¬ï¼‰ã€‚ä¹‹æ‰€ä»¥é€‰æ‹©å¦‚æ­¤å¤šçš„æ ·æœ¬ï¼Œæ˜¯å› ä¸ºåœ¨å¼€å§‹ä»¿çœŸæ—¶ï¼Œæˆ‘ä»¬éš¾ä»¥æ–­å®šæ¢å¤å…¨éƒ¨æµåœºéœ€è¦å¤šå°‘çš„æ•°æ®é‡ã€‚

|           | æ•°æ®æ ¼å¼                                                     | æ•°æ®ç»„æˆ                                                     |
| --------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **è¾“å…¥ ** | è®¾è®¡å˜é‡ï¼šå˜é‡å°ºå¯¸ 10<br/>ç©ºé—´åæ ‡ï¼šé€šé“æ•° 2ï¼Œç©ºé—´å°ºå¯¸                792Ã—40 | è®¾è®¡å˜é‡ï¼šåŒ…æ‹¬äº”ä¸ªå¯å˜å‡ ä½•å˜é‡ã€ä¸€ä¸ªè¡¨å¾çº³ç±³æµä½“çš„å·¥è´¨ç‰¹æ€§çš„å‚æ•°ã€ä¸¤ä¸ªè¡¨å¾è¾¹ç•Œæ¡ä»¶çš„å·¥å†µå‚æ•°ï¼›æ­¤å¤–ï¼Œè¿˜åŒ…æ‹¬ä¸¤ä¸ªå¸¸æ•°ï¼Œåˆ†åˆ«ä¸ºå…¥å£æ¸©åº¦ *T*= 293K å’Œå‡ºå£å‹å¼º*P* =100KPa ã€‚<br/>ç©ºé—´åæ ‡ï¼šåŒ…å«ä¸‰ä¸ªæ–¹å‘ï¼Œå³x, yå’Œzã€‚æ³¨æ„ï¼Œæœ¬æ–‡åªå–xå’Œyä¸¤ä¸ªæ–¹å‘ã€‚ |
| **è¾“å‡º**  | ç‰©ç†åœºï¼šé€šé“æ•°4 ï¼Œç©ºé—´å°ºå¯¸792Ã—40     <br/>æ€§èƒ½å‚æ•°ï¼šç©ºé—´å°ºå¯¸ 2 | 4ä¸ªç‰©ç†åœºï¼šå‹å¼ºåœºpï¼Œæ¸©åº¦åœºtï¼Œxæ–¹å‘çš„é€Ÿåº¦åœºuï¼Œyæ–¹å‘çš„é€Ÿåº¦åœºvã€‚<br/>2ä¸ªæ€§èƒ½å‚æ•°ï¼šNusseltæ•°å’ŒFanningæ‘©æ“¦å› å­ |

â€‹		å¾®é€šé“çš„*x*, *y*æ–¹å‘çš„å°ºå¯¸åˆ†åˆ«ä¸º[0, 05] *Î¼*må’Œ [0, *a*] *Î¼*mï¼Œ*a* = [130, 270] *Î¼*mã€‚8ä¸ªè®¾è®¡å˜é‡çš„å–å€¼å¦‚ä¸‹æ‰€ç¤ºã€‚

| è®¾è®¡å˜é‡ | *R*e | *Ï†*/% | *l*3/Î¼m | *R*1/*Î¼*m | *R*2/*Î¼*m | *Î´*1 | *Î´*2 | *q*/WÂ·m2 |
| :------: | :--: | :---: | :-----: | :-------: | :-------: | :--: | :--: | :------: |
| **ä¸‹é™** |  43  |  0.1  |   30    |    10     |    10     | -15  | -15  |  10,000  |
| **ä¸Šé™** | 1000 |  10   |   150   |    70     |    70     |  28  |  28  | 100,000  |



### 1.2 æ•°æ®é‡‡æ ·

â€‹		å»ºç«‹HeatDatasetç±»ï¼Œç»§æ‰¿äº 'Dataset' ç±»ï¼Œä»¥ä¾¿è¿›è¡Œæ•°æ®åŠ è½½ã€é¢„å¤„ç†ç­‰ã€‚åœ¨åˆå§‹åŒ–æ–¹æ³•ä¸­ï¼Œmodeå¯å–0ï¼Œ1ï¼Œ2ï¼Œåˆ†åˆ«ä»£è¡¨è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•ã€‚'sample_size' , 'training_size' å’Œ 'test_size' åˆ†åˆ«è¡¨ç¤ºé€‰å–æ ·æœ¬æ•°é‡ï¼Œè®­ç»ƒé›†å æ ·æœ¬æ•°é‡çš„æ¯”ä¾‹å’Œæµ‹è¯•é›†å æ ·æœ¬æ•°é‡çš„æ¯”ä¾‹ã€‚

```python
class HeatDataset(Dataset):
    TRAIN = 0
    VALID = 1
    TEST = 2
    def __init__(self, file, mode=0, sampler={'sample_mode': 'all', 'sample_size': 0.6},
                 shuffle=True, training_size=0.8, test_size=0.1):
```

### 1.3 æ•°æ®è¯»å–

â€‹		å‡½æ•°data_readå®ç°äº†æ•°æ®è¯»å–çš„åŠŸèƒ½ï¼Œé€šè¿‡MatLoaderåŠ è½½æˆ‘ä»¬çš„matæ•°æ®æ–‡ä»¶ã€‚é¦–å…ˆï¼Œå–matæ–‡ä»¶å†…çš„ 'data' çš„ç¬¬ä¸€ç»´ï¼ˆå³batchsizeï¼‰å®šä¹‰ä¸ºlengthã€‚å°†è¯»å–åˆ°çš„ 'data' å­—æ®µçš„æ•°æ®æŒ‰ç…§ self.shuffle_idx çš„é¡ºåºè¿›è¡Œé‡æ–°æ’åˆ—ï¼Œç„¶åå°†ç»“æœèµ‹ç»™äº† self.data.designã€‚ ' grids' å’Œ ' field'åˆ†åˆ«ä¸ºç©ºé—´åæ ‡å’Œæ¸©åº¦åœºï¼Œåœ¨è¯»å–æ•°æ®æ—¶ï¼Œæˆ‘ä»¬å‡åœ¨xæ–¹å‘å°†åˆ†è¾¨ç‡é™ä½ä¸ºåŸå§‹æ•°æ®çš„1å€ï¼Œä¸” åæ ‡åªå–å‰ä¸¤ä¸ªæ–¹å‘xå’Œyã€‚'target' æŒ‡æ€§èƒ½å‚æ•°ï¼Œæ˜¯ Nu å’Œ f åœ¨æœ€åä¸€ä¸ªç»´åº¦ä¸Šæ‹¼æ¥èµ·æ¥çš„ã€‚

```python
    def data_read(self):
        class data:
            pass
        reader = MatLoader(self.file, to_paddle=False, to_cuda=False, to_float=True)
        self.data = data
        self.data.length = reader.read_field('data').shape[0]
        if not self.shuffle:
            self.shuffle_idx = np.arange(self.data.length)
        else:
            self.shuffle_idx = np.random.permutation(self.data.length)

        self.data.design = reader.read_field('data')[self.shuffle_idx]
        
        #æ³¨æ„åŸå§‹æ•°æ®åœ¨xæ–¹å‘åˆ†è¾¨ç‡é™ä½äº†1å€
        self.data.coords = reader.read_field('grids')[:, ::2, :, :2][self.shuffle_idx]  
        self.data.fields = reader.read_field('field')[:, ::2, :, :][self.shuffle_idx]
        self.data.target = np.concatenate((reader.read_field('Nu'), reader.read_field('f')), axis=-1)  	 
        [self.shuffle_idx]
```



### 1.4 æ•°æ®é›†åˆ’åˆ†

â€‹		å‡½æ•°data_splitå®ç°äº†æ•°æ®é›†åˆ’åˆ†çš„åŠŸèƒ½ã€‚è®­ç»ƒé›†å¤§å°= æ ·æœ¬å¤§å°*è®­ç»ƒé›†æ¯”ä¾‹ï¼ŒéªŒè¯é›†å¤§å°=max(æ ·æœ¬å¤§å° -ï¼ˆ1-è®­ç»ƒé›†å¤§å° - æµ‹è¯•é›†å¤§å°ï¼‰, 200)ï¼Œ æµ‹è¯•é›†å¤§å°=éªŒæ ·æœ¬å¤§å°-è®­ç»ƒé›†å¤§å°-éªŒè¯é›†å¤§å°ã€‚

â€‹		å½“mode=0æ—¶ï¼Œè®­ç»ƒé›†ä¸­è®¾è®¡å˜é‡ï¼Œåæ ‡ï¼Œç‰©ç†åœºï¼Œæ€§èƒ½å‚æ•°å‡å–ä»0åˆ°åˆ—è¡¨çš„ç¬¬self.train_lenä¸ªæ•°æ®ï¼›å½“mode=1æ—¶ï¼ŒéªŒè¯é›†ä¸­è®¾è®¡å˜é‡ï¼Œåæ ‡ï¼Œç‰©ç†åœºï¼Œæ€§èƒ½å‚æ•°å‡å–ä»è®­ç»ƒé›†dçš„ç¬¬self.train_lenä¸ªæ•°æ®åˆ°è®­ç»ƒé›†åŠ éªŒè¯é›†å¤§å°ä¸ªæ•°æ®ï¼›å½“mode=2æ—¶ï¼Œæµ‹è¯•é›†ä¸­è®¾è®¡å˜é‡ï¼Œåæ ‡ï¼Œç‰©ç†åœºï¼Œæ€§èƒ½å‚æ•°å‡å–ä»ä»åˆ—è¡¨çš„å€’æ•°ç¬¬ self.test_len ä¸ªå…ƒç´ å¼€å§‹ï¼Œä¸€ç›´åˆ°åˆ—è¡¨çš„æœ€åä¸€ä¸ªå…ƒç´ ã€‚

```python
    def data_split(self):

        self.train_len = int(self.data.length * self.training_size)                     #è®­ç»ƒé›†é•¿åº¦
        self.valid_len = max(int(self.data.length * (1 - self.training_size - self.test_size)), 200)  #                                                    éªŒè¯é›†é•¿åº¦
        self.test_len = self.data.length - self.train_len - self.valid_len         #æµ‹è¯•é›†é•¿åº¦

        if self.mode == self.TRAIN:        #mode=0
            self.data.design = self.data.design[:self.train_len]
            self.data.coords = self.data.coords[:self.train_len]
            self.data.fields = self.data.fields[:self.train_len]
            self.data.target = self.data.target[:self.train_len]
            self.data.length = self.train_len
        elif self.mode == self.VALID:      #mode=1
            self.data.design = self.data.design[self.train_len:self.train_len + self.valid_len]
            self.data.coords = self.data.coords[self.train_len:self.train_len + self.valid_len]
            self.data.fields = self.data.fields[self.train_len:self.train_len + self.valid_len]
            self.data.target = self.data.target[self.train_len:self.train_len + self.valid_len]
            self.data.length = self.valid_len
        elif self.mode == self.TEST:       #mode=2
            self.data.design = self.data.design[-self.test_len:]
            self.data.coords = self.data.coords[-self.test_len:]
            self.data.fields = self.data.fields[-self.test_len:]
            self.data.target = self.data.target[-self.test_len:]
            self.data.length = self.test_len

```

### 1.5 æ•°æ®åŠ è½½

â€‹		å»ºç«‹ 'HeatDataLoader' ç±»ï¼Œç»§æ‰¿è‡ª  DataLoader ç±»ï¼Œç”¨äºæ‰¹é‡åŠ è½½æ•°æ®é›†ã€‚åœ¨åˆå§‹åŒ–æ–¹æ³•ä¸­ï¼Œ'batchsize' ä¸ºæ¯ä¸ªæ‰¹æ¬¡æ”¾å…¥æ ·æœ¬æ•°ï¼Œ'shuffle' ä¸ºæ˜¯å¦åœ¨æ¯ä¸ªè®­ç»ƒæ—¶æ‰“ä¹±æ ·æœ¬çš„é¡ºåºï¼Œ'drop_last' ä¸ºæ˜¯å¦ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸æ»¡è¶³ä¸€ä¸ªæ ·æœ¬æ•°é‡çš„æ‰¹æ¬¡ã€‚

```python
class HeatDataLoader(DataLoader):

    def __init__(self, dataset, batch_size=1, shuffle=False, drop_last=False,):
        super(HeatDataLoader, self).__init__(dataset=dataset, batch_size=batch_size, shuffle=shuffle,
                                             drop_last=drop_last)
```

### 1.6 æ•°æ®å½’ä¸€åŒ–

â€‹		ä¸ºäº†æ¶ˆé™¤è®¾è®¡å˜é‡ã€ç©ºé—´åæ ‡ã€ç‰©ç†åœºå’Œæ€§èƒ½å‚æ•°ä¹‹é—´çš„é‡çº²å·®è·ï¼Œæˆ‘ä»¬å¯¹æ•°æ®è¿›è¡Œäº†å½’ä¸€åŒ–æ“ä½œã€‚åœ¨ä»£ç ä¸­ï¼Œæä¾›äº†ä¸¤ç§å½’ä¸€åŒ–æ–¹æ³•ï¼Œåˆ†åˆ«ä¸º 'min-max' å½’ä¸€åŒ–å’Œ 'mean-std' å½’ä¸€åŒ–ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œç”±äºç‰©ç†åœºçš„æœ€å¤§å€¼å’Œæœ€å°å€¼å…·æœ‰æ˜æ˜¾çš„ç‰©ç†æ„ä¹‰ï¼Œå› æ­¤æˆ‘ä»¬é‡‡ç”¨ 'min-max' å½’ä¸€åŒ–æ–¹å¼ã€‚

```python
class DataNormer(object):
    """
        åœ¨æœ€åä¸€ä¸ªç»´åº¦è¿›è¡Œå½’ä¸€åŒ–
    """
    def __init__(self, data, method="min-max", axis=None):
   
        if isinstance(data, str):
            if os.path.isfile(data):
                try:
                    self.load(data)
                except:
                    raise ValueError("the savefile format is not supported!")
            else:
                raise ValueError("the file does not exist!")
        elif type(data) is np.ndarray:
            if axis is None:
                axis = tuple(range(len(data.shape) - 1))
            self.method = method
            if method == "min-max":
                self.max = np.max(data, axis=axis)
                self.min = np.min(data, axis=axis)

            elif method == "mean-std":
                self.mean = np.mean(data, axis=axis)
                self.std = np.std(data, axis=axis)
        elif type(data) is paddle.Tensor:
            if axis is None:
                axis = tuple(range(len(data.shape) - 1))
            self.method = method
            if method == "min-max":
                self.max = np.max(data.numpy(), axis=axis)
                self.min = np.min(data.numpy(), axis=axis)

            elif method == "mean-std":
                self.mean = np.mean(data.numpy(), axis=axis)
                self.std = np.std(data.numpy(), axis=axis)
        else:
            raise NotImplementedError("the data type is not supported!")


    def norm(self, x):           #æ•°æ®å½’ä¸€åŒ–
        """
            è¾“å…¥å¼ é‡
            å‚æ•° x: è¾“å…¥å¼ é‡
            è¿”å› x: è¾“å‡ºå¼ é‡
        """
        if paddle.is_tensor(x):
            if self.method == "min-max":
                x = 2 * (x - paddle.to_tensor(self.min, place=x.place)) \
                    / (paddle.to_tensor(self.max, place=x.place) - paddle.to_tensor(self.min, place=x.place) + 1e-10) - 1
            elif self.method == "mean-std":
                x = (x - paddle.to_tensor(self.mean, place=x.place)) / (paddle.to_tensor(self.std + 1e-10, place=x.place))
        else:
            if self.method == "min-max":
                x = 2 * (x - self.min) / (self.max - self.min + 1e-10) - 1
            elif self.method == "mean-std":
                x = (x - self.mean) / (self.std + 1e-10)

        return x

    def back(self, x):          #æ•°æ®åå½’ä¸€åŒ–
        """
            input tensors
            param x: input tensors
            return x: output tensors
        """
        if paddle.is_tensor(x):
            if self.method == "min-max":
                x = (x + 1) / 2 * (paddle.to_tensor(self.max)
                                   - paddle.to_tensor(self.min) + 1e-10) + paddle.to_tensor(self.min)
            elif self.method == "mean-std":
                x = x * (paddle.to_tensor(self.std + 1e-10)) + paddle.to_tensor(self.mean)
        else:
            if self.method == "min-max":
                x = (x + 1) / 2 * (self.max - self.min + 1e-10) + self.min
            elif self.method == "mean-std":
                x = x * (self.std + 1e-10) + self.mean
        return x
```



## 3. ç½‘ç»œè®­ç»ƒåŠéªŒè¯

### 3.1 è®­ç»ƒè¿‡ç¨‹

â€‹		å®šä¹‰train_epochå‡½æ•°ï¼Œæ—¨åœ¨å¯¹ç½‘ç»œè¿›è¡Œè®­ç»ƒã€‚ç½‘ç»œçš„è¾“å…¥ä¸ºè®¾è®¡å˜é‡å’Œç©ºé—´åæ ‡ï¼Œç‰©ç†åœºæŸå¤±é€‰æ‹©loss_funcï¼Œå³yamlæ–‡ä»¶ 'basic_config' çš„loss_nameå¯¹åº”çš„æŸå¤±ç±»å‹ã€‚æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é€‰æ‹©MSEæŸå¤±ä½œä¸ºç‰©ç†åœºæ€»æŸå¤±ã€‚

```python
     def train_epoch(self, train_loader):
        self.network.train()
        for data in train_loader:
            design, coords, fields, _ = data
            self.optimizer.clear_grad()
            fields_ = self.network(design, coords)
            loss = self.loss_func(fields_, fields)
            loss.backward()
            self.optimizer.step()
        self.scheduler.step()
```

### 3.2 ç½‘ç»œéªŒè¯

â€‹		å®šä¹‰valid_epochå‡½æ•°ï¼Œæ—¨åœ¨å¯¹ç½‘ç»œè¿›è¡ŒéªŒè¯ã€‚é™¤äº†å¯¹ç‰©ç†åœºçš„æ±‚è§£ï¼Œæˆ‘ä»¬è¿˜æ·»åŠ äº†å¯¹æ€§èƒ½å‚æ•°çš„æ±‚è§£ï¼Œå³è¾“å…¥ç‰©ç†åœºã€ç©ºé—´åæ ‡å’Œè®¾è®¡å˜é‡ã€‚ç‰©ç†åœºæ€»æŸå¤±å’Œæ€§èƒ½å‚æ•°æ€»æŸå¤±å‡é‡‡ç”¨MSEæŸå¤±ï¼Œè€Œfields_metricå’Œtarget_metricåˆ†åˆ«è¡¨ç¤ºå››ä¸ªç‰©ç†åœºå’Œä¸¤ä¸ªæ€§èƒ½å‚æ•°çš„å„è‡ªæŸå¤±ï¼Œå…·ä½“è®¡ç®—æ–¹å¼ç”±PhysicsLpLossç±»é€šè¿‡æ±‚è§£ç›¸å¯¹èŒƒæ•°æ¥å®Œæˆè®¡ç®—ã€‚å…¶ä¸­ï¼Œfields_metricé‡‡ç”¨äºŒé˜¶èŒƒæ•°ï¼Œtarget_metricé‡‡ç”¨ä¸€é˜¶èŒƒæ•°ï¼ŒåŒæ—¶è€ƒè™‘åˆ°æ€§èƒ½å‚æ•°fæœ‰æ¥è¿‘äº0çš„æƒ…å†µï¼Œå› æ­¤ä½¿ç”¨relative=Falseã€‚

```python
    def valid_epoch(self, data_loader):

        log_metric = {'target': [], 'fields': []}
        log_loss = {'target': [], 'fields': []}

        self.network.eval()
        with paddle.no_grad():
            for data in data_loader:
                design, coords, fields, _ = data

                fields_ = self.network(design, coords)
                fields_loss = self.loss_func(fields_, fields).item()

                design = data_loader.design_back(design)
                coords = data_loader.coords_back(coords)
                fields = data_loader.fields_back(fields)
                fields_ = data_loader.fields_back(fields_)

                target = self.characteristic(fields, coords, design)
                target_ = self.characteristic(fields_, coords, design)

                target_loss = self.loss_func(target_, target).item()

                fields_metric = self.fields_metric(fields_, fields).cpu().numpy()
                target_metric = self.target_metric(target_, target).cpu().numpy()

                log_metric['fields'].append(fields_metric)
                log_metric['target'].append(target_metric)

                log_loss['fields'].append(fields_loss)
                log_loss['target'].append(target_loss)

        log_metric['fields'] = np.concatenate(log_metric['fields'], axis=0)
        log_metric['target'] = np.concatenate(log_metric['target'], axis=0)

        log_loss['fields'] = np.array(log_loss['fields'])
        log_loss['target'] = np.array(log_loss['target'])

        return log_metric, log_loss
```

### 	3.3 ç½‘ç»œæ¨ç†æµ‹è¯•

â€‹		å®šä¹‰inferå‡½æ•°ï¼Œæ—¨åœ¨ä½¿ç”¨æ•°æ®åŠ è½½å™¨ data_loaderæ¥è·å–å¯¹åº”çš„è®­ç»ƒã€éªŒè¯æˆ–æµ‹è¯•æ•°æ®ï¼ŒåŒæ—¶ä½¿ç”¨ data_name æ¥ç”Ÿæˆå¯¹åº”æ–‡ä»¶ã€‚æ­¤éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†æ‰€æœ‰è®­ç»ƒæ•°æ®ã€éªŒè¯æ•°æ®å’Œæµ‹è¯•æ•°æ®åˆ†åˆ«è¿›è¡Œæ±‚è§£ï¼Œå¾—åˆ°å„è‡ªçš„ç‰©ç†åœºã€æ€§èƒ½å‚æ•°çš„çœŸå®å€¼ã€é¢„æµ‹å€¼ä»¥åŠæµ‹è¯•å€¼ã€‚åŒæ—¶ï¼Œå¯è§†åŒ–äºŒç»´ç‰©ç†åœºäº‘å›¾å’Œæ€§èƒ½å‚æ•°å›å½’å›¾ã€‚

```python
 def infer(self, data_loader, data_name, show_nums=20):
```



### 	3.4 è®­ç»ƒè¿‡ç¨‹

â€‹		å®šä¹‰trainå‡½æ•°ï¼Œæ—¨åœ¨åˆ©ç”¨ train_loader å’Œ valid_loader æ¥è·å–è®­ç»ƒæ•°æ®å’ŒéªŒè¯æ•°æ®ï¼Œå°†epochã€è®­ç»ƒæ—¶é—´ã€éªŒè¯æ—¶é—´ã€ç‰©ç†åœºæ€»æŸå¤±ã€æ€§èƒ½å‚æ•°æ€»æŸå¤±ã€4ä¸ªç‰©ç†åœºå’Œ2ä¸ªæ€§èƒ½å‚æ•°å„è‡ªçš„æŸå¤±å…¨éƒ¨ä¿å­˜åœ¨loghistory.pkl æ–‡ä»¶å†…ï¼Œå¹¶æŠŠæ¨¡å‹å’Œè®­ç»ƒçš„å‚æ•°ï¼Œä¾‹å¦‚epochã€æ•°æ®é…ç½®ã€ä¼˜åŒ–å™¨ã€å­¦ä¹ ç‡ã€ç½‘ç»œå‚æ•°ç­‰ä¿¡æ¯å…¨éƒ¨ä¿å­˜åœ¨last_model.pdparamsæ–‡ä»¶ä¸­ã€‚åŒæ—¶ï¼Œå¯¹æŸå¤±å‡½æ•°è¿›è¡Œå¯è§†åŒ–ã€‚

```python
  def train(self, train_loader, valid_loader):
```



### 3.5 æ€§èƒ½å‚æ•°ç§¯åˆ†æ±‚è§£

â€‹		å®šä¹‰ Characteristic ç±»ï¼Œæ—¨åœ¨åˆ©ç”¨ç¥ç»ç®—å­ç½‘ç»œé¢„æµ‹çš„ç‰©ç†åœºï¼Œé€šè¿‡ç§¯åˆ†æ±‚è§£çš„æ–¹å¼ï¼Œå®ç°å¯¹æ€§èƒ½å‚æ•° Nu å’Œ f çš„é¢„æµ‹ã€‚å‡½æ•°get_parameters_of_nano å®ç°äº†å¯¹çº³ç±³æµä½“çš„ç‰©ç†æ€§è´¨ï¼ˆçƒ­å¯¼ç‡ã€æ¯”çƒ­ã€å¯†åº¦ã€åŠ¨åŠ›ç²˜åº¦ï¼‰è¿›è¡Œæ±‚è§£ï¼›å‡½æ•° cal_f ã€cal_tb ã€cal_tw å®ç°äº†å¯¹æ§åˆ¶æ–¹ç¨‹ä¸­åå¯¼æ•°çš„æ±‚è§£ã€‚

```python
class Characteristic(nn.Layer):

    def __init__(self):
        super(Characteristic, self).__init__()

    def get_parameters_of_nano(self, per):
        lamda_water = 0.597
        Cp_water = 4182.
        rho_water = 998.2
        miu_water = 9.93e-4

        lamda_al2o3 = 36.
        Cp_al2o3 = 773.
        rho_al2o3 = 3880.

        rho = per * rho_al2o3 + (1. - per) * rho_water
        Cp = ((1. - per) * rho_water * Cp_water + per * rho_al2o3 * Cp_al2o3) / rho
        miu = miu_water * (123. * per ** 2. + 7.3 * per + 1)
        DELTA = ((3. * per - 1.) * lamda_al2o3 + (2. - 3. * per) * lamda_water) ** 2
        DELTA = DELTA + 8. * lamda_al2o3 * lamda_water
        lamda = 0.25 * ((3 * per - 1) * lamda_al2o3 + (2 - 3 * per) * lamda_water + paddle.sqrt(DELTA))

        return lamda, Cp, rho, miu           

    def cal_f(self, X, Y, P):
        F_inn = (P[:, 0, 1:] + P[:, 0, 0:-1]) / 2
        F_out = (P[:, -1, 1:] + P[:, -1, 0:-1]) / 2

        dy_inn = Y[:, 0, 1:] - Y[:, 0, 0:-1]
        dy_out = Y[:, -1, 1:] - Y[:, -1, 0:-1]

        D_P = paddle.sum(F_inn * dy_inn, axis=(1,)) / paddle.sum(dy_inn, axis=(1,)) \
              - paddle.sum(F_out * dy_out, axis=(1,)) / paddle.sum(dy_out, axis=(1,))

        return D_P

    def cal_tb(self, X, Y, T):
        F_T = T[:, :, :]

        dxx = X[:, :-1, :] - X[:, 1:, :]
        dxy = Y[:, :-1, :] - Y[:, 1:, :]
        dyx = X[:, :, 1:] - X[:, :, :-1]
        dyy = Y[:, :, 1:] - Y[:, :, :-1]

        ds1 = paddle.abs(dxx[:, :, :-1] * dyy[:, 1:] - dxy[:, :, :-1] * dyx[:, 1:]) / 2
        ds2 = paddle.abs(dxx[:, :, 1:] * dyy[:, :-1] - dxy[:, :, 1:] * dyx[:, :-1]) / 2
        ds = ds1 + ds2

        M_T = (F_T[:, 1:, 1:] + F_T[:, 1:, :-1] + F_T[:, :-1, 1:] + F_T[:, :-1, :-1]) / 4

        Tb = paddle.sum(ds * M_T, axis=(1, 2)) / paddle.sum(ds, axis=(1, 2))

        return Tb

    def cal_tw(self, X, Y, T):
        up_t = T[:, :, -1]
        down_t = T[:, :, 0]

        temp = paddle.sqrt((X[:, :-1, -1] - X[:, 1:, -1]) ** 2 + (Y[:, :-1, -1] - Y[:, 1:, -1]) ** 2)
        up_dl = paddle.zeros_like(X[:, :, 0])
        up_dl[:, 1:-1] = (temp[:, :-1] + temp[:, 1:]) / 2
        up_dl[:, 0] = temp[:, 0] / 2
        up_dl[:, -1] = temp[:, -1] / 2

        temp = paddle.sqrt((X[:, :-1, 0] - X[:, 1:, 0]) ** 2 + (Y[:, :-1, 0] - Y[:, 1:, 0]) ** 2)
        down_dl = paddle.zeros_like(X[:, :, 0])
        down_dl[:, 1:-1] = (temp[:, :-1] + temp[:, 1:]) / 2
        down_dl[:, 0] = temp[:, 0] / 2
        down_dl[:, -1] = temp[:, -1] / 2

        Tw = ((paddle.sum(up_t * up_dl, axis=1) + paddle.sum(down_t * down_dl, axis=1))
              / paddle.sum(up_dl + down_dl, axis=1))

        return Tw

    def forward(self, field, grid, design):
        D = float(2 * 200 * 1e-6)  # æ°´åŠ›ç›´å¾„
        L = float(3500 * 1e-6)  # é€šé“é•¿åº¦

        per = design[:, 3]  # Al2O3ä½“ç§¯åˆ†æ•°
        Re = design[:, 0]  # Reynaldo
        hflex = design[:, 2]  # çƒ­æµå¯†åº¦

        lamda, Cp, rho, miu = self.get_parameters_of_nano(per)

        I_ext = 121
        X = grid[:, I_ext:-I_ext, :, 0]
        Y = grid[:, I_ext:-I_ext, :, 1]
        P = field[:, I_ext:-I_ext, :, 0]
        T = field[:, I_ext:-I_ext, :, 1]

        Tw = self.cal_tw(X, Y, T)
        Tb = self.cal_tb(X, Y, T)
        h = hflex / (paddle.abs(Tw - Tb) + 1e-8)  # ä¿è¯æ•°å€¼ç¨³å®šæ€§
        Nu = h * D / lamda

        vel = Re * miu / rho / D
        Dp = self.cal_f(X, Y, P)
        Fan = Dp * D / 2 / L / vel / vel / rho

        result = paddle.stack((Nu, Fan), axis=1)

        return result
```



## 4. ç½‘ç»œæ¶æ„åŠè®­ç»ƒæ–¹æ³•

### 4.1 FNO

#### 4.1.1 ç®€ä»‹

FNO: Fourier Neural Operator

Reference: [3] Fourier Neural Operator for Parametric Partial Differential Equations

Github: https://github.com/zongyi-li/fourier_neural_operator

<img src="C:\Users\zt\Desktop\3.jpg" style="zoom:50%;" />

#### 4.1.2 ä»£ç è¯´æ˜

â€‹		æ­¤ç¥ç»ç®—å­ç½‘ç»œçš„ç½‘ç»œç»“æ„ï¼Œå¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œä¸»è¦ç”±å…¨è¿æ¥å±‚å’Œå‚…é‡Œå¶å±‚ç»„æˆã€‚é¦–å…ˆï¼Œé‡‡ç”¨è®¾è®¡å˜é‡å’Œç©ºé—´åæ ‡ä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡å…¨è¿æ¥å±‚å°†è¾“å…¥é€šé“ä»4æå‡åˆ°é«˜ç»´é€šé“32ã€‚å…¶æ¬¡ï¼Œé€šè¿‡4ä¸ªå‚…é‡Œå¶å±‚ï¼ˆæ¯å±‚åŒ…æ‹¬ä¸¤ä¸ªä¸åŒçš„æ“ä½œï¼Œå³é¡¶éƒ¨æ“ä½œå’Œåº•éƒ¨æ“ä½œï¼‰ï¼Œå¾—åˆ°å‚…é‡Œå¶å±‚çš„è¾“å‡ºã€‚å†é€šè¿‡ä¸¤ä¸ªå…¨è¿æ¥å±‚ï¼Œå®ç°ç©ºé—´ç»´åº¦ä»398Ã—42åˆ°396Ã—40çš„å˜åŒ–ï¼Œä¸”é€šé“æ•°ä»42 å…ˆå¢åŠ åˆ°128ï¼Œå¹¶æœ€ç»ˆå†é™ç»´å›åˆ°ç›®æ ‡ç»´åº¦4ã€‚

```python
class FNO2d(nn.Layer):
    """
        2ç»´FNOç½‘ç»œ
    """

    def __init__(self, in_dim, out_dim, modes=(8, 8), width=32, depth=4, steps=1, padding=2,                     activation='gelu', dropout=0.0):
        super(FNO2d, self).__init__()

        """
        1.é€šè¿‡å…¨è¿æ¥å±‚ self.fc0 å°†è¾“å…¥é€šé“æå‡åˆ°æƒ³è¦çš„ç»´åº¦.
        2. 4å±‚ç§¯åˆ†ç®—å­ u' = (W + K)(u).
            W ç”± self.wå®šä¹‰; Kç”±self.conv å®šä¹‰.
        3.  é€šè¿‡ self.fc1 å’Œ self.fc2å°†ç©ºé—´ç»´åº¦é™ä½å›ç›®æ ‡ç»´åº¦.

        è¾“å…¥:  10 timesteps + 2 locations (u(t-10, x, y), ..., u(t-1, x, y),  x, y)
        è¾“å…¥å½¢çŠ¶: (batchsize, x, y, c)
        è¾“å‡º: ä¸‹ä¸€ä¸ª timestep çš„è§£
        è¾“å‡ºå½¢çŠ¶: (batchsize, x, y, c)
        """
        self.in_dim = in_dim
        self.out_dim = out_dim
        self.modes = modes
        self.width = width
        self.depth = depth
        self.steps = steps
        self.padding = padding            # å¦‚æœè¾“å…¥æ˜¯éå‘¨æœŸæ€§çš„ï¼Œåˆ™è¿›è¡Œå¡«å……
        self.activation = activation
        self.dropout = dropout
        self.fc0 = nn.Linear(steps * in_dim + 2, self.width)

        self.convs = nn.LayerList()
        for i in range(self.depth):
            self.convs.append(
                SpectralConv2d(self.width, self.width, self.modes, activation=self.activation,                   dropout=self.dropout))

        self.fc1 = nn.Linear(self.width, 128)
        self.fc2 = nn.Linear(128, out_dim)

    def forward(self, x, grid):
        """
        å‰å‘è®¡ç®—
        """
        if len(x.shape) != len(grid.shape):
            repeat_times = paddle.to_tensor([1]+grid.shape[1:-1]+[1], dtype='int32')
            x = paddle.tile(x[:, None, None, :], repeat_times=repeat_times)

        x = paddle.concat((x, grid), axis=-1)
        x = self.fc0(x)
        x = x.transpose((0, 3, 1, 2))

        if self.padding != 0:
            x = F.pad(x, [0, self.padding, 0, self.padding])  # å¦‚æœè¾“å…¥æ˜¯éå‘¨æœŸæ€§çš„ï¼Œåˆ™è¿›è¡Œå¡«å……

        for i in range(self.depth):
            x = self.convs[i](x)

        if self.padding != 0:
            x = x[..., :-self.padding, :-self.padding]
        x = x.transpose((0, 2, 3, 1))  # å¦‚æœè¾“å…¥æ˜¯éå‘¨æœŸæ€§çš„ï¼Œåˆ™è¿›è¡Œå¡«å……
        x = self.fc1(x)
        x = F.gelu(x)
        x = self.fc2(x)
        return x
```

#### 4.1.3 å‚æ•°é…ç½®

â€‹		ä¸‹é¢æˆ‘ä»¬åˆ—å‡ºäº†FNOçš„å…·ä½“è®­ç»ƒå‚æ•°å’Œç½‘ç»œå‚æ•°è®¾ç½®ã€‚å…¶ä¸­ï¼Œä¸ºäº†æ›´æ¸…æ™°åœ°å±•ç¤ºå‚æ•°é…ç½®ï¼Œæˆ‘ä»¬åœ¨'basic_config' ä¸­åˆ—å‡ºçš„è®­ç»ƒå‚æ•°ï¼Œå‡ä¸ºé’ˆå¯¹æ¯ä¸ªç‰¹å®šæ¨¡å‹è®­ç»ƒæ—¶è®¾ç½®çš„ï¼Œé™¤æ­¤å¤–å‰©ä½™çš„åŸºæœ¬è®­ç»ƒå‚æ•°è®¾ç½®ï¼Œè§ 1.1 ä¸­çš„ 'basic_config' ã€‚

```python
basic_config:
  learning_rate: 1.e-3
  weight_decay: 1.e-9
  learning_beta:
    - 0.99
    - 0.99
  learning_milestones:
    - 200
    - 300
    - 400
  learning_gamma: 0.1

FNO_model:
  in_dim: 10         #è¾“å…¥ç»´åº¦
  out_dim: 4         #è¾“å‡ºç»´åº¦
  modes: 12          #æ¨¡æ€
  width: 32          #ç½‘ç»œå®½åº¦
  depth: 4           #ç½‘ç»œæ·±åº¦
  steps: 1          
  padding: 2         #å¡«å……
  activation: 'gelu' #æ¿€æ´»å‡½æ•°
  dropout: 0.0       #æ­£åˆ™åŒ–æ–¹æ³•ï¼Œå‡å°‘è¿‡æ‹Ÿåˆ
```



### 4.2 U-Net

#### 4.1.1 ç®€ä»‹

U-Net: æ˜¯ä¸€ç§å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ–¹æ³•ï¼Œç”¨äºå›¾åƒåˆ†å‰²ä»»åŠ¡çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œæœ€åˆç”±Olaf Ronnebergerç­‰äººåœ¨2015å¹´æå‡ºã€‚å®ƒçš„åå­—æ¥æºäºå…¶Uå½¢çŠ¶çš„ç½‘ç»œç»“æ„ã€‚

Reference: [4] U-Net: Convolutional Networks for Biomedical Image Segmentation

<img src="C:\Users\zt\AppData\Roaming\Typora\typora-user-images\image-20231014094749732.png" style="zoom: 50%;" />



#### 4.1.2 ä»£ç è¯´æ˜

â€‹		åœ¨æœ¬ç½‘ç»œæ¶æ„ä¸­ï¼Œæˆ‘ä»¬è®¾ç½®ç½‘ç»œå®½åº¦ä¸º32ï¼Œæ·±åº¦ä¸º6ï¼Œé‡‡ç”¨GELUæ¿€æ´»å‡½æ•°ã€‚*E*nä¸»è¦ç”±ä¸€ä¸ª2Ã—2çš„æœ€å¤§æ± åŒ–å±‚å’Œ2ä¸ª3Ã—3çš„å·ç§¯å±‚ç»„æˆã€‚é¦–å…ˆå°†è¾“å…¥å›¾åƒè¿›è¡Œä¸Šé‡‡æ ·ï¼Œé€šè¿‡æ’å€¼æ”¹å˜å¼ é‡å°ºå¯¸ï¼Œå°†ç©ºé—´å°ºå¯¸ä»396Ã—40å‡å°ä¸º256Ã—40ï¼Œå†ç»è¿‡2ä¸ª3Ã—3çš„å·ç§¯å±‚ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è¿›è¡Œ2Ã—2çš„ä¸‹é‡‡æ ·ï¼Œå†è¿›è¡Œ2ä¸ª3Ã—3çš„å·ç§¯å±‚ï¼Œä¿è¯ç©ºé—´å°ºå¯¸ä¸å˜åŒ–ï¼Œå¦‚æ­¤å¾ªç¯5æ¬¡åï¼Œé€šé“æ•°ä»æœ€åˆçš„32å˜ä¸ºäº†1024ï¼Œå›¾åƒç©ºé—´å°ºå¯¸ä»396Ã—40å˜ä¸º8Ã—2ã€‚åŒç†ï¼Œ*B*oç»“æ„ä¸*E*nç›¸ä¼¼ï¼Œä½†åªè¿›è¡Œä¸€æ¬¡æœ€å¤§æ± åŒ–å’Œä¸¤æ¬¡å·ç§¯ï¼Œæ­¤æ—¶ï¼Œé€šé“æ•°å˜ä¸º2048ï¼Œç©ºé—´å°ºå¯¸å˜ä¸º4Ã—1ã€‚*D*eä¸»è¦ç”±ä¸€ä¸ª2Ã—2çš„è½¬ç½®å·ç§¯ï¼Œä¸€ä¸ªç‰¹å¾èåˆï¼Œä¸¤ä¸ª3Ã—3çš„å·ç§¯å±‚ç»„æˆã€‚å¦‚æ­¤6æ¬¡å¾ªç¯åï¼Œé€šé“æ•°ä»1024å˜å›ä¸º32ï¼Œç©ºé—´å°ºå¯¸ä»8Ã—2å˜å›ä¸º256Ã—64ã€‚æœ€åï¼Œæˆ‘ä»¬ç»è¿‡ä¸€ä¸ªä¸Šé‡‡æ ·ï¼Œå°†ç©ºé—´å°ºå¯¸è°ƒæ•´ä¸º396Ã—40ï¼Œé€šé“æ•°å‡å°‘ä¸º4ï¼Œæœ€åå†ç»è¿‡ä¸€ä¸ª3Ã—3çš„å·ç§¯å±‚åï¼Œè¾“å‡ºæˆ‘ä»¬æƒ³è¦çš„4ä¸ªç‰©ç†åœºã€‚

```python
class UNet2d(nn.Layer):
    """
        2ç»´UNet
    """

    def __init__(self, in_sizes: tuple, out_sizes: tuple, width=32, depth=4, steps=1, activation='gelu',
                 dropout=0.0):
        """
        :param in_sizes: (H_in, W_in, C_in)
        :param out_sizes: (H_out, W_out, C_out)
        :param width: hidden dim, int
        :param depth: hidden layers, int
        """
        super(UNet2d, self).__init__()

        self.in_sizes = in_sizes[:-1]
        self.out_sizes = out_sizes[:-1]
        self.in_dim = in_sizes[-1]
        self.out_dim = out_sizes[-1]
        self.width = width
        self.depth = depth
        self.steps = steps

        self._input_sizes = [0, 0]
        self._input_sizes[0] = max(2 ** math.floor(math.log2(self.in_sizes[0])), 2 ** depth)
        self._input_sizes[1] = max(2 ** math.floor(math.log2(self.in_sizes[1])), 2 ** depth)


        self.interp_in = Interp2dUpsample(in_dim=steps*self.in_dim + 2, out_dim=self.in_dim, activation=activation,
                                          dropout=dropout, interp_size=self._input_sizes, conv_block=True)
        self.encoders = nn.LayerList()
        for i in range(self.depth):
            if i == 0:
                self.encoders.append(
                    Conv2dResBlock(self.in_dim, width, basic_block=True, activation=activation, dropout=dropout))
            else:
                self.encoders.append(nn.Sequential(nn.MaxPool2D(2),
                                                   Conv2dResBlock(2 ** (i - 1) * width, 2 ** i * width,
                                                                  basic_block=True, activation=activation,
                                                                  dropout=dropout)))

        self.bottleneck = nn.Sequential(nn.MaxPool2D(2),
                                        Conv2dResBlock(2 ** i * width, 2 ** i * width * 2, basic_block=True,
                                                       activation=activation, dropout=dropout))

        self.decoders = nn.LayerList()
        self.upconvs = nn.LayerList()

        for i in range(self.depth, 0, -1):
            self.decoders.append(
                Conv2dResBlock(2 ** i * width, 2 ** (i - 1) * width, activation=activation,
                               basic_block=True, dropout=dropout))
            self.upconvs.append(
                DeConv2dBlock(2 ** i * width, 2 ** (i - 1) * width, 2 ** (i - 1) * width, activation=activation,
                              dropout=dropout))

        self.conv1 = Conv2dResBlock(in_dim=width, out_dim=self.out_dim, basic_block=False, activation=activation,
                                    dropout=dropout)

        self.interp_out = Interp2dUpsample(in_dim=self.out_dim, out_dim=self.out_dim, interp_size=self.out_sizes,
                                           conv_block=False, activation=activation, dropout=dropout)

        self.conv2 = nn.Conv2D(self.out_dim, self.out_dim, kernel_size=3, stride=1, padding=1)

    def forward(self, x, grid):
        """
        forward computation
        """
        if len(x.shape) != len(grid.shape):
            repeat_times = paddle.to_tensor([1]+grid.shape[1:-1]+[1], dtype='int32')
            x = paddle.tile(x[:, None, None, :], repeat_times=repeat_times)

        x = paddle.concat((x, grid), axis=-1)
        x = x.transpose([0, 3, 1, 2])
        enc = []
        enc.append(self.interp_in(x))
        for i in range(self.depth):
            enc.append(self.encoders[i](enc[-1]))

        x = self.bottleneck(enc[-1])

        for i in range(self.depth):
            x = self.upconvs[i](x)
            x = paddle.concat((x, enc[-i - 1]), axis=1)
            x = self.decoders[i](x)

        x = self.interp_out(self.conv1(x))
        x = self.conv2(x)
        return x.transpose([0, 2, 3, 1])
```

#### 4.1.3 å‚æ•°é…ç½®

â€‹	ä¸‹é¢æˆ‘ä»¬åˆ—å‡ºäº†U-Netçš„å…·ä½“è®­ç»ƒå‚æ•°å’Œç½‘ç»œå‚æ•°è®¾ç½®ã€‚å…¶ä¸­ï¼Œä¸ºäº†æ›´æ¸…æ™°åœ°å±•ç¤ºå‚æ•°é…ç½®ï¼Œæˆ‘ä»¬åœ¨'basic_config' ä¸­åˆ—å‡ºçš„è®­ç»ƒå‚æ•°ï¼Œå‡ä¸ºé’ˆå¯¹æ¯ä¸ªç‰¹å®šæ¨¡å‹è®­ç»ƒæ—¶è®¾ç½®çš„ï¼Œé™¤æ­¤å¤–å‰©ä½™çš„åŸºæœ¬è®­ç»ƒå‚æ•°è®¾ç½®ï¼Œè§ 1.1 ä¸­çš„ 'basic_config' ã€‚

```python
basic_config:
  total_epoch: 500
  learning_rate: 1.e-4
  weight_decay: 1.e-12
  learning_beta:
    - 0.7
    - 0.9
  learning_milestones:
    - 300
    - 400
    - 500
  learning_gamma: 0.1

CNN_model:
  in_sizes: [396, 40, 10]
  out_sizes: [396, 40, 4]
  width: 32
  depth: 6
  steps: 1    # Number of steps to unroll, should be 1 in this problem
  activation: 'gelu'
  dropout: 0.0
```



### 4.3 FNN

#### 4.1.1 ç®€ä»‹

FNN: Fully Neural Network

Reference: [5] A physics-informed deep learning framework for inversion and surrogate modeling in solid mechanics

<img src="C:\Users\zt\AppData\Roaming\Typora\typora-user-images\image-20231014093121334.png" style="zoom: 50%;" />

#### 4.1.2 ä»£ç è¯´æ˜

â€‹		åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤šä¸ªå…¨è¿æ¥å±‚ï¼Œæ¯ä¸ªå…¨è¿æ¥å±‚æ¥é¢„æµ‹ä¸€ä¸ªç‰©ç†åœºã€‚è¾“å…¥ç»´åº¦ä¸ºè®¾è®¡å˜é‡+ç©ºé—´åæ ‡=10ï¼Œç½‘ç»œå®½åº¦ä¸º64ï¼Œæ·±åº¦ä¸º5ï¼Œè¾“å‡ºç»´åº¦æ˜¯4ï¼Œå³å‹å¼ºåœºã€æ¸©åº¦åœºã€é€Ÿåº¦åœº*u*å’Œ*v*ã€‚ 

```python
class FcnMulti(nn.Layer):
    def __init__(self, in_dim, out_dim, planes: list, steps=1, activation="gelu"):
        # =============================================================================
        #     Inspired by Haghighat Ehsan, et all.
        #     "A physics-informed deep learning framework for inversion and surrogate modeling in solid mechanics"
        #     Computer Methods in Applied Mechanics and Engineering.
        # =============================================================================
        super(FcnMulti, self).__init__()
        self.planes = [steps * in_dim + 2,] + planes + [out_dim]
        self.active = activation_dict[activation]

        self.layers = nn.LayerList()
        for j in range(self.planes[-1]):
            layer = []
            for i in range(len(self.planes) - 2):
                layer.append(nn.Linear(self.planes[i], self.planes[i + 1]))
                layer.append(self.active)
            layer.append(nn.Linear(self.planes[-2], 1))
            self.layers.append(nn.Sequential(*layer))
        self.reset_parameters()

    def reset_parameters(self):
        """
        weight initialize
        """
        for m in self.sublayers():
            if isinstance(m, nn.Linear):
                # nn.init.xavier_normal_(m.weight, gain=1)
                w = params_initial('xavier_normal', shape=m.weight.shape)
                m.weight.set_value(w)
                b = params_initial('constant', shape=m.bias.shape)
                m.bias.set_value(b)

    def forward(self, x, grid):
        """
        forward compute
        :param in_var: (batch_size, ..., input_dim)
        """

        if len(x.shape) != len(grid.shape):
            repeat_times = paddle.to_tensor([1]+grid.shape[1:-1]+[1], dtype='int32')
            x = paddle.tile(x[:, None, None, :], repeat_times=repeat_times)

        in_var = paddle.concat((x, grid), axis=-1)

        y = []
        for i in range(self.planes[-1]):
            y.append(self.layers[i](in_var))
        return paddle.concat(y, axis=-1)
```

#### 4.1.3 å‚æ•°é…ç½®

â€‹	ä¸‹é¢æˆ‘ä»¬åˆ—å‡ºäº†FNNçš„å…·ä½“è®­ç»ƒå‚æ•°å’Œç½‘ç»œå‚æ•°è®¾ç½®ã€‚å…¶ä¸­ï¼Œä¸ºäº†æ›´æ¸…æ™°åœ°å±•ç¤ºå‚æ•°é…ç½®ï¼Œæˆ‘ä»¬åœ¨'basic_config' ä¸­åˆ—å‡ºçš„è®­ç»ƒå‚æ•°ï¼Œå‡ä¸ºé’ˆå¯¹æ¯ä¸ªç‰¹å®šæ¨¡å‹è®­ç»ƒæ—¶è®¾ç½®çš„ã€‚é™¤æ­¤å¤–å‰©ä½™çš„åŸºæœ¬è®­ç»ƒå‚æ•°è®¾ç½®ï¼Œè§ 1.1 ä¸­çš„ 'basic_config' ã€‚

```python
basic_config:
 
  total_epoch: 800
  learning_rate: 1.e-3
  weight_decay: 0.
  learning_beta:
    - 0.7
    - 0.9
  learning_milestones:
    - 600
    - 700
    - 800
  learning_gamma: 0.1


MLP_model:
  in_dim: 10
  out_dim: 4
  steps: 1
  planes:
    - 64
    - 64
    - 64
    - 64
    - 64
  activation: 'gelu'
```



### 4.4 DeepONet

#### 4.1.1 ç®€ä»‹

DeepONet: deep operator network

Reference: [6] Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators

<img src="C:\Users\zt\AppData\Roaming\Typora\typora-user-images\image-20231014093758871.png" style="zoom: 80%;" />

#### 4.1.2 ä»£ç è¯´æ˜

â€‹		DeepONetæ˜¯æ·±åº¦ç¥ç»ç®—å­ç½‘ç»œï¼Œç”±ä¸»å¹²ç½‘ç»œTrunk netå’Œåˆ†æ”¯ç½‘ç»œBranch netç»„æˆï¼Œé€šè¿‡æœ€å°åŒ–ç›®æ ‡ç®—å­ä¸ç»™å®šçš„ç¥ç»ç½‘ç»œä¹‹é—´çš„è¯¯å·®æ¥å®ç°å¯¹å¤æ‚ç‰¹å¾çš„å‡†ç¡®é¢„æµ‹ã€‚å…¶ä¸­ï¼Œè¯¥ç½‘ç»œæ”¯æŒæ­å»ºå¤šä¸ªBranch netï¼Œä¾‹å¦‚è¾¹ç•Œæ¡ä»¶æ˜¯åˆ†å¸ƒå¼çš„è¾“å…¥ã€‚åœ¨å¯¹åœ¨å¯¹è¯¥ç½‘ç»œè¿›è¡Œè®­ç»ƒæ—¶ï¼Œæˆ‘ä»¬Branch netçš„è¾“å…¥ç»´åº¦ä¸ºåŒ…å«10ä¸ªè®¾è®¡å˜é‡çš„åˆ—è¡¨ï¼ŒTrunk netçš„è¾“å…¥ç»´åº¦ä¸ºåŒ…å«2ä¸ªç©ºé—´åæ ‡çš„å¼ é‡ï¼Œåˆ†åˆ«è®¾ç½®äºŒè€…çš„éšè—å±‚æ•°ä¸º[64, 64, 64, 64, 64]å’Œ [64, 64, 64, 64]ã€‚

```python
class DeepONetMulti(nn.Layer):
    # =============================================================================
    #     Inspired by L. Lu, J. Pengzhan, G.E. Karniadakis.
    #     "DeepONet: Learning nonlinear operators for identifying differential equations based on
    #     the universal approximation theorem of operators".
    #     arXiv:1910.03193v3 [cs.LG] 15 Apr 2020.
    # =============================================================================
    def __init__(self, in_dim: int, out_dim: int, operator_dims: list,
                 planes_branch: list, planes_trunk: list, activation='gelu'):
        """
        :param in_dim: int, the coordinates dim for trunk net
        :param operator_dims: listï¼Œthe operate dims list for each branch net
        :param out_dim: int, the predicted variable dims
        :param planes_branch: list, the hidden layers dims for branch net
        :param planes_trunk: list, the hidden layers dims for trunk net
        :param operator_dims: listï¼Œthe operate dims list for each branch net
        :param activation: activation function
        """
        super(DeepONetMulti, self).__init__()

        self.branches = nn.LayerList() # åˆ†æ”¯ç½‘ç»œ
        self.trunks = nn.LayerList() # ä¸»å¹²ç½‘ç»œ
        for dim in operator_dims:
            self.branches.append(MLP([dim] + planes_branch, activation=activation))# FcnSingleæ˜¯ä»basic_layersé‡Œå¯¼å…¥çš„
        for _ in range(out_dim):
            self.trunks.append(MLP([in_dim] + planes_trunk, activation=activation))

        self.reset_parameters()

    def reset_parameters(self):
        """
        weight initialize
        """
        for m in self.sublayers():
            if isinstance(m, nn.Linear):
                # nn.init.xavier_normal_(m.weight, gain=1)
                w = params_initial('xavier_normal', shape=m.weight.shape)
                m.weight.set_value(w)
                b = params_initial('constant', shape=m.bias.shape)
                m.bias.set_value(b)

    def forward(self, u_vars, y_var, size_set=False):
        """
        forward compute
        :param u_vars: tensor list[(batch_size, ..., operator_dims[0]), (batch_size, ..., operator_dims[1]), ...]
        :param y_var: (batch_size, ..., input_dim)
        :param size_set: bool, true for standard inputs, false for reduce points number in operator inputs
        """
        B = 1.

        if not isinstance(u_vars, list or tuple):
            u_vars = [u_vars,]

        for u_var, branch in zip(u_vars, self.branches):
            B *= branch(u_var)

        if not size_set:
            B_size = list(y_var.shape[1:-1])
            for i in range(len(B_size)):
                B = B.unsqueeze(1)
            B = paddle.tile(B, [1, ] + B_size + [1, ])

        out_var = []
        for trunk in self.trunks:
            T = trunk(y_var)
            out_var.append(paddle.sum(B * T, axis=-1)) # ç”¨è¿™ç§æ–¹å¼å®ç°ä¸¤ä¸ªç½‘ç»œçš„ä¹˜ç§¯
        out_var = paddle.stack(out_var, axis=-1)
        return out_var
```



#### 4.1.3 å‚æ•°é…ç½®

â€‹	ä¸‹é¢æˆ‘ä»¬åˆ—å‡ºäº†DeepONetçš„å…·ä½“è®­ç»ƒå‚æ•°å’Œç½‘ç»œå‚æ•°è®¾ç½®ã€‚å…¶ä¸­ï¼Œä¸ºäº†æ›´æ¸…æ™°åœ°å±•ç¤ºå‚æ•°é…ç½®ï¼Œæˆ‘ä»¬åœ¨'basic_config' ä¸­åˆ—å‡ºçš„è®­ç»ƒå‚æ•°ï¼Œå‡ä¸ºé’ˆå¯¹æ¯ä¸ªç‰¹å®šæ¨¡å‹è®­ç»ƒæ—¶è®¾ç½®çš„ï¼Œé™¤æ­¤å¤–å‰©ä½™çš„åŸºæœ¬è®­ç»ƒå‚æ•°è®¾ç½®ï¼Œè§ 1.1 ä¸­çš„ 'basic_config' ã€‚

```python
basic_config:
  total_epoch: 800
  learning_rate: 1.e-2
  weight_decay: 0.
  learning_beta:
    - 0.7
    - 0.9
  learning_milestones:
    - 600
    - 700
    - 800
  learning_gamma: 0.1


DON_model:
  in_dim: 2                # note: deeponet in_dim ä¸ºç©ºé—´åæ ‡
  out_dim: 4
  operator_dims:           # note: deeponet operator_dims ä¸ºè®¾è®¡å˜é‡
    - 10
  planes_branch:
    - 64
    - 64
    - 64
    - 64
    - 64
  planes_trunk:
    - 64
    - 64
    - 64
    - 64
  activation: 'gelu'
```



### 4.5 Transformer

#### 4.1.1 ç®€ä»‹

referenceï¼š[7] Choose a Transformer: Fourier or Galerkin

githubï¼šhttps://github.com/scaomath/galerkin-transformer

| å½¢å¼     | Attentionè®¡ç®—æ–¹å¼                                            |                                                              |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Fourier  | <img src="C:\Users\zt\Desktop\or2\a62efc6b06d3441492c507b1c381521fe474fcf9eed94ee6b53a7c36cd24184b.jpg" style="zoom: 50%;" /> | <img src="C:\Users\zt\Desktop\or2\2.jpg" style="zoom: 33%;" /> |
| Galerkin | <img src="C:\Users\zt\Desktop\or2\3.jpg" style="zoom: 50%;" /> | <img src="C:\Users\zt\Desktop\or2\4.jpg" style="zoom: 33%;" /> |

#### 4.1.2 ä»£ç è¯´æ˜

â€‹		æ”¹è¿›çš„attentionæœºåˆ¶æ ¸å¿ƒç®—æ³•å®ç°è¿‡ç¨‹ï¼šï¼ˆåŒ…æ‹¬fourier,galerkin,linear,softmaxï¼‰

```python
class SimpleAttention(nn.Layer):
    '''
    The attention is using a vanilla (QK^T)V or Q(K^T V) with no softmax
    For an encoder layer, the tensor size is slighly different from the official pytorch implementation

    attn_types:
        - fourier: integral, local
        - galerkin: global
        - linear: standard linearization
        - softmax: classic softmax attention

    In this implementation, output is (N, L, E).
    batch_first will be added in the next version of PyTorch: https://github.com/pytorch/pytorch/pull/55285

    Reference: code base modified from
    https://nlp.seas.harvard.edu/2018/04/03/attention.html
    - added xavier init gain
    - added layer norm <-> attn norm switch
    - added diagonal init

    In https://github.com/lucidrains/linear-attention-transformer/blob/master/linear_attention_transformer/linear_attention_transformer.py
    the linear attention in each head is implemented as an Einstein sum
    attn_matrix = paddle.einsum('bhnd,bhne->bhde', k, v)
    attn = paddle.einsum('bhnd,bhde->bhne', q, attn_matrix)
    return attn.reshape(*q.shape)
    here in our implementation this is achieved by a slower transpose+matmul
    but can conform with the template Harvard NLP gave
    '''

    def __init__(self, n_head, d_model,
                 pos_dim: int = 1,
                 attention_type='fourier',
                 dropout=0.1,
                 xavier_init=1e-4,
                 diagonal_weight=1e-2,
                 symmetric_init=False,
                 norm_add=False,
                 norm_type='layer',
                 eps=1e-5):
        super(SimpleAttention, self).__init__()
        assert d_model % n_head == 0  # n_head å¯è¢«d_modelæ•´é™¤
        self.attention_type = attention_type
        self.d_k = d_model // n_head
        self.n_head = n_head
        self.pos_dim = pos_dim
        self.linears = nn.LayerList(
            [copy.deepcopy(nn.Linear(d_model, d_model)) for _ in range(3)])
        self.xavier_init = xavier_init
        self.diagonal_weight = diagonal_weight
        self.symmetric_init = symmetric_init
        if self.xavier_init > 0:
            self._reset_parameters()
        self.norm_add = norm_add
        self.norm_type = norm_type
        if norm_add:
            self._get_norm(eps=eps)

        if pos_dim > 0:
            self.fc = nn.Linear(d_model + n_head * pos_dim, d_model)

        self.attn_weight = None
        self.dropout = nn.Dropout(dropout)

    def forward(self, query, key, value, pos=None, mask=None, weight=None):
        """
        forward compute
        :param query: (batch, seq_len, d_model)
        :param key: (batch, seq_len, d_model)
        :param value: (batch, seq_len, d_model)
        """
        if mask is not None:
            mask = mask.unsqueeze(1)

        bsz = query.shape[0]
        if weight is not None:
            query, key = weight * query, weight * key

        query, key, value = \
            [layer(x).reshape((bsz, -1, self.n_head, self.d_k)).transpose((0, 2, 1, 3))
             for layer, x in zip(self.linears, (query, key, value))]

        if self.norm_add:
            if self.attention_type in ['linear', 'galerkin', 'global']:
                if self.norm_type == 'instance':
                    key, value = key.transpose((0, 1, 3, 2)), value.transpose((0, 1, 3, 2))

                key = paddle.stack(
                    [norm(x) for norm, x in
                     zip(self.norm_K, (key[:, i, ...] for i in range(self.n_head)))], axis=1)
                value = paddle.stack(
                    [norm(x) for norm, x in
                     zip(self.norm_V, (value[:, i, ...] for i in range(self.n_head)))], axis=1)

                if self.norm_type == 'instance':
                    key, value = key.transpose((0, 1, 3, 2)), value.transpose((0, 1, 3, 2))
            else:
                if self.norm_type == 'instance':
                    key, query = key.transpose((0, 1, 3, 2)), query.transpose((0, 1, 3, 2))

                key = paddle.stack(
                    [norm(x) for norm, x in
                     zip(self.norm_K, (key[:, i, ...] for i in range(self.n_head)))], axis=1)
                query = paddle.stack(
                    [norm(x) for norm, x in
                     zip(self.norm_Q, (query[:, i, ...] for i in range(self.n_head)))], axis=1)

                if self.norm_type == 'instance':
                    key, query = key.transpose((0, 1, 3, 2)), query.transpose((0, 1, 3, 2))

        if pos is not None and self.pos_dim > 0:
            assert pos.shape[-1] == self.pos_dim
            pos = pos.unsqueeze(1)
            pos = pos.tile([1, self.n_head, 1, 1])
            query, key, value = [paddle.concat([pos, x], axis=-1)
                                 for x in (query, key, value)]

        if self.attention_type in ['linear', 'galerkin', 'global']:
            x, self.attn_weight = linear_attention(query, key, value,
                                                   mask=mask,
                                                   attention_type=self.attention_type,
                                                   dropout=self.dropout)
        else:
            x, self.attn_weight = vanilla_attention(query, key, value,
                                                    mask=mask,
                                                    attention_type=self.attention_type,
                                                    dropout=self.dropout)

        out_dim = self.n_head * self.d_k if pos is None else self.n_head * \
                                                             (self.d_k + self.pos_dim)
        att_output = x.transpose((0, 2, 1, 3)).reshape((bsz, -1, out_dim))

        if pos is not None and self.pos_dim > 0:
            att_output = self.fc(att_output)

        return att_output, self.attn_weight
```

#### 4.1.3 å‚æ•°é…ç½®

â€‹		ä¸‹é¢æˆ‘ä»¬åˆ—å‡ºäº†Transformerçš„å…·ä½“è®­ç»ƒå‚æ•°å’Œç½‘ç»œå‚æ•°è®¾ç½®ã€‚å…¶ä¸­ï¼Œä¸ºäº†æ›´æ¸…æ™°åœ°å±•ç¤ºå‚æ•°é…ç½®ï¼Œæˆ‘ä»¬åœ¨'basic_config' ä¸­åˆ—å‡ºçš„è®­ç»ƒå‚æ•°ï¼Œå‡ä¸ºé’ˆå¯¹æ¯ä¸ªç‰¹å®šæ¨¡å‹è®­ç»ƒæ—¶è®¾ç½®çš„ï¼Œé™¤æ­¤å¤–å‰©ä½™çš„åŸºæœ¬è®­ç»ƒå‚æ•°è®¾ç½®ï¼Œè§ 1.1 ä¸­çš„ 'basic_config' ã€‚

```python
basic_config:
  total_epoch: 800
  learning_rate: 1.e-3
  weight_decay: 0.
  learning_beta:
    - 0.9
    - 0.99
  learning_milestones:
    - 600
    - 700
    - 800
  learning_gamma: 0.1

TNO_model:
  node_feats: 10    #è®¾è®¡å˜é‡
  pos_dim: 2        #ç©ºé—´åæ ‡
  n_targets: 4      #ç›®æ ‡å˜é‡
  n_hidden: 64
  num_feat_layers: 0   #ç‰¹å¾æå–æ›¾æ•°ç›®
  num_encoder_layers: 4  #ç¼–ç å±‚æ•°ç›®
  n_head: 4
  normalizer: False
  dim_feedforward: 128
  residual_type: add
  attention_type: galerkin
  attn_activation: gelu
  feat_extract_type: None
  xavier_init: 0.01
  diagonal_weight: 0.01
  symmetric_init: False
  layer_norm: True
  attn_norm: False
  norm_eps: 0.00001
  batch_norm: False
  return_attn_weight: False
  return_latent: False
  decoder_type: ifft2
  last_activation: True
  freq_dim: 32
  num_regressor_layers: 2
  regressor_activation: gelu
  fourier_modes: 16
  spacial_dim: 2    #ç©ºé—´ç»´åº¦
  spacial_fc: False  #æ˜¯å¦æ·»åŠ ç©ºé—´ç»´åº¦åˆ°è¾“å…¥ç»´åº¦
  dropout: 0.0
  encoder_dropout: 0.0
  decoder_dropout: 0.0
  ffn_dropout: 0.0
```



### 4.6 è®­ç»ƒæ–¹æ³•

â€‹		äº”ç§ç¥ç»ç®—å­ç½‘ç»œå‡é‡‡ç”¨ç›¸åŒçš„è®­ç»ƒæ–¹å¼ï¼Œå³Adamä¼˜åŒ–å™¨ã€‚FNOã€U-Netã€FNNã€DeepONetå’ŒTransformerçš„è®­ç»ƒæ­¥é•¿åˆ†åˆ«é‡‡ç”¨400ï¼Œ500ï¼Œ800ï¼Œ800å’Œ800ä¸ªEpochã€‚æŸå¤±å‡½æ•°é‡‡ç”¨ä¸‰ç§æ–¹å¼ï¼šè®­ç»ƒæ€»æŸå¤±å’Œæ€§èƒ½å‚æ•°æŸå¤±é‡‡ç”¨*L*2ï¼ˆMSEï¼‰æŸå¤±ï¼›ç‰©ç†åœºæŸå¤±é‡‡ç”¨å¹³å‡ç›¸å¯¹è¯¯å·®å’Œæœ€å¤§ç›¸å¯¹è¯¯å·®ã€‚æŸå¤±å‡½æ•°å…·ä½“å…¬å¼å¦‚ä¸‹æ‰€ç¤ºï¼š

| æŸå¤±å‡½æ•°                   | å…¬å¼                                                         |
| -------------------------- | ------------------------------------------------------------ |
| **ç‰©ç†åœºåŠæ€§èƒ½å‚æ•°æ€»æŸå¤±** | <img src="C:\Users\zt\Desktop\L2.png" style="zoom: 50%;" />  |
| **ç‰©ç†åœº  ï¼šå¹³å‡ç›¸å¯¹è¯¯å·®** | <img src="C:\Users\zt\Desktop\MAX.png" style="zoom: 50%;" /> |
| **ç‰©ç†åœº  ï¼šæœ€å¤§ç›¸å¯¹è¯¯å·®** | <img src="C:\Users\zt\Desktop\MEA.png" style="zoom:50%;" />  |
| **æ€§èƒ½å‚æ•°ï¼šç›¸å¯¹è¯¯å·®**     | <img src="C:\Users\zt\Desktop\canshu.png" style="zoom:50%;" /> |



## 5. ç»“æœåˆ†æ

- ä¸‹è¿°æ‰€æœ‰ç»“æœä»…å±•ç¤ºäº†éƒ¨åˆ†åˆ†æï¼Œè¯¦ç»†çš„ç»“è®ºè¯·å‚è§æ–‡ç« ä¸­ã€‚

â€‹		ä¸‹å›¾å±•ç¤ºäº†äº”ç§ç¥ç»ç®—å­ç½‘ç»œæ¨¡å‹åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­çš„æ€»ç‰©ç†åœºæŸå¤±ã€æ¯ä¸ªç‰©ç†åœºçš„æŸå¤±åŠæ€§èƒ½å‚æ•°æ”¶æ•›æƒ…å†µã€‚

| æŸå¤±æ”¶æ•›æ›²çº¿                 |                                                              |
| ---------------------------- | ------------------------------------------------------------ |
| **ç‰©ç†åœºæ€»æŸå¤±**             | ![](C:\Users\zt\AppData\Roaming\Typora\typora-user-images\image-20231014084848857.png) |
| **æ€§èƒ½å‚æ•°æ€»æŸå¤±**           | <img src="C:\Users\zt\AppData\Roaming\Typora\typora-user-images\image-20231014084933092.png" style="zoom:23%;" /> |
| **å››ä¸ªç‰©ç†åœºçš„å¹³å‡ç›¸å¯¹è¯¯å·®** | <img src="C:\Users\zt\AppData\Roaming\Typora\typora-user-images\image-20231014085044410.png" style="zoom:23%;" /> |
| **å››ä¸ªç‰©ç†åœºçš„æœ€å¤§ç›¸å¯¹è¯¯å·®** | <img src="C:\Users\zt\AppData\Roaming\Typora\typora-user-images\image-20231014085134461.png" style="zoom:23%;" /> |

â€‹		ä¸ºäº†æ›´ç›´è§‚åœ°æ¯”è¾ƒä¸åŒæ¨¡å‹åœ¨é¢„æµ‹ç‰©ç†åœºä¸­çš„åå·®ï¼Œæˆ‘ä»¬é€‰æ‹©60%çš„æ ·æœ¬é‡ï¼Œå¯¹äº”ç§ç¥ç»ç®—å­ç½‘ç»œæ¨¡å‹é¢„æµ‹ç‰©ç†åœºçš„æœ€å¤§ç›¸å¯¹è¯¯å·®å’Œå¹³å‡ç›¸å¯¹è¯¯å·®è¿›è¡Œå¯¹æ¯”ã€‚

| FMAXD     | <img src="C:\Users\zt\Desktop\image-20231014073800647.png"  /> |
| --------- | ------------------------------------------------------------ |
| **FMEAD** | ![](C:\Users\zt\Desktop\image-20231014073818625.png)         |

â€‹		ä¸‹å›¾å±•ç¤ºäº†äº”ç§ç¥ç»ç®—å­ç½‘ç»œé¢„æµ‹çš„å‹åŠ›åœºpã€æ¸©åº¦åœºtã€é€Ÿåº¦åœºuå’Œvçš„çœŸå®åœºã€é¢„æµ‹åœºå’Œè¯¯å·®åˆ†å¸ƒã€‚

| é¢„æµ‹å…¨å±€ç‰©ç†åœº  |                                                            |
| --------------- | ---------------------------------------------------------- |
| **FNO**         | <img src="C:\Users\zt\Desktop\1.jpg" style="zoom: 50%;" /> |
| **U-Net**       | <img src="C:\Users\zt\Desktop\2.jpg" style="zoom:50%;" />  |
| **FNN**         | <img src="C:\Users\zt\Desktop\4.jpg" style="zoom:50%;" />  |
| **DeepONet**    | <img src="C:\Users\zt\Desktop\5.jpg" style="zoom:50%;" />  |
| **Transformer** | <img src="C:\Users\zt\Desktop\7.jpg" style="zoom:50%;" />  |

â€‹		æ­¤å¤„ï¼Œæˆ‘ä»¬åªæ”¾ç½®é¢„æµ‹æ•ˆæœæœ€å¥½çš„Tranformerå’Œæœ€å·®çš„DeepONetçš„ç‰©ç†åœºå±€éƒ¨æ”¾å¤§å›¾ã€‚

| é¢„æµ‹å±€éƒ¨ç‰©ç†åœº  |                                                             |
| --------------- | ----------------------------------------------------------- |
| **Transformer** | <img src="C:\Users\zt\Desktop\11.jpg" style="zoom: 33%;" /> |
| **DeepONet**    | <img src="C:\Users\zt\Desktop\22.jpg" style="zoom:33%;" />  |

â€‹			ä¸ºäº†æ›´ç›´è§‚åœ°å¯è§†åŒ–å¾®é€šé“å†…æ¯ä¸ªç‰©ç†å˜é‡çš„å˜åŒ–è¶‹åŠ¿ï¼Œå¹¶å¯¹æ¯”ä¸åŒç¥ç»ç®—å­ç½‘ç»œæ¨¡å‹åœ¨æ•æ‰å±€éƒ¨ç‰©ç†åœºç»†èŠ‚çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬é€‰æ‹©äº†ä¸‰ä¸ªç‰¹å®šçš„ä½ç½®è¿›è¡Œåˆ†æï¼Œåˆ†ææ–¹æ³•å—æ–‡ç« [8]å¯å‘ã€‚è¿™äº›ä½ç½®åˆ†åˆ«ä½äºé è¿‘å¾®é€šé“çš„ä¸Šå£ã€ä¸‹å£å’Œä¸­é—´åŒºåŸŸã€‚æ›²çº¿Uä½äºè·ç¦»ä¸Šå£6.30 *Î¼*må¤„ï¼Œæ›²çº¿Dä½äºè·ç¦»ä¸‹å£6.30 *Î¼*må¤„ï¼Œæ›²çº¿Mä½äºå¾®é€šé“ä¸­å¿ƒï¼Œè·ç¦»ä¸Šä¸‹å£é¢ç­‰è·2.5 mmã€‚æ³¨æ„ï¼šä¸ºäº†æ›´æ¸…æ¥šåœ°åæ˜ ä¸‰ä¸ªä¸åŒé€šé“ä½ç½®çš„ç‰©ç†å˜é‡å˜åŒ–è¶‹åŠ¿åŠä¸åŒç¥ç»ç®—å­ç½‘ç»œæ¨¡å‹çš„å±€éƒ¨é¢„æµ‹æ•ˆæœï¼Œæˆ‘ä»¬åœ¨æ­¤å¤„åªä»»é€‰ä¸¤ç§ç¥ç»ç®—å­ç½‘ç»œFNOå’ŒU-Netä¸çœŸå®å€¼è¿›è¡Œå¯¹æ¯”ã€‚å…¶ä¸­è“çº¿ä¸ºçœŸå®å€¼ï¼Œçº¢åœˆã€é»‘åœˆåˆ†åˆ«è¡¨ç¤ºFNOå’ŒU-Netçš„é¢„æµ‹å€¼ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸¤ç§ç¥ç»ç®—å­ç½‘ç»œæ¨¡å‹é¢„æµ‹çš„ç‰©ç†å˜é‡ä¸å®é™…å€¼åŸºæœ¬ä¸€è‡´ï¼Œä½†åœ¨éçŸ©å½¢åŒºåŸŸçš„å‡¹æ§½å’Œå‡¸æ§½é™„è¿‘ï¼ŒU-Netå¾ˆéš¾æ•æ‰åˆ°å±€éƒ¨ç»†å¾®ç‰¹å¾ï¼Œå› æ­¤é¢„æµ‹è¯¯å·®è¾ƒå¤§ã€‚	

| å¾®é€šé“å†…éƒ¨ä¸åŒä½ç½®çš„ç‰©ç†é‡åˆ†æ | æ›²çº¿D                                                        | æ›²çº¿M                                                        | æ›²çº¿U                                                        |
| ------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **å‹å¼ºp**                      | <img src="C:\Users\zt\AppData\Roaming\Typora\typora-user-images\image-20231014075733456.png" style="zoom: 20%;" /> | <img src="C:\Users\zt\AppData\Roaming\Typora\typora-user-images\image-20231014080010767.png" style="zoom:25%;" /> | <img src="C:\Users\zt\AppData\Roaming\Typora\typora-user-images\image-20231014080149573.png" style="zoom:20%;" /> |
| **æ¸©åº¦t**                      | <img src="C:\Users\zt\AppData\Roaming\Typora\typora-user-images\image-20231014080331865.png" style="zoom:30%;" /> | <img src="C:\Users\zt\AppData\Roaming\Typora\typora-user-images\image-20231014080452411.png" style="zoom:30%;" /> | <img src="C:\Users\zt\AppData\Roaming\Typora\typora-user-images\image-20231014080547025.png" style="zoom:25%;" /> |
| **é€Ÿåº¦u**                      | <img src="C:\Users\zt\AppData\Roaming\Typora\typora-user-images\image-20231014080705280.png" style="zoom:25%;" /> | <img src="C:\Users\zt\AppData\Roaming\Typora\typora-user-images\image-20231014080809356.png" style="zoom:25%;" /> | <img src="C:\Users\zt\AppData\Roaming\Typora\typora-user-images\image-20231014080850810.png" style="zoom:25%;" /> |
| **é€Ÿåº¦v**                      | <img src="C:\Users\zt\AppData\Roaming\Typora\typora-user-images\image-20231014080930512.png" style="zoom:25%;" /> | <img src="C:\Users\zt\AppData\Roaming\Typora\typora-user-images\image-20231014081017587.png" style="zoom:25%;" /> | <img src="C:\Users\zt\AppData\Roaming\Typora\typora-user-images\image-20231014081147769.png" style="zoom:25%;" /> |

â€‹		åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æˆåŠŸåœ°é¢„æµ‹äº†æµä½“çš„ç‰©ç†åœºï¼Œå¹¶ä»¥ç©ºé—´ç§¯åˆ†çš„å½¢å¼æå–äº†æœ‰æ•ˆè¡¨å¾æµä½“ä¼ çƒ­æ€§èƒ½çš„çƒ­ç‰©æ€§å‚æ•°*Nu*å’Œ*f*ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œ*x*è½´è¡¨ç¤ºå®é™…å€¼ï¼Œ*y*è½´è¡¨ç¤ºé¢„æµ‹å€¼ã€‚æ•£ç‚¹è¶Šæ¥è¿‘*y* *= x*çº¿ï¼Œé¢„æµ‹ç»“æœè¶Šå‡†ç¡®ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ*Nu* å’Œ *f* çš„é¢„æµ‹è¯¯å·®åˆ†åˆ«åœ¨6%å’Œ5%ä»¥å†…ï¼Œè¿™è¯å®äº†æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥è¾ƒé«˜åœ°ç²¾åº¦åœ°å®ç°å¯¹æµåŠ¨æ¢çƒ­æ€§èƒ½å‚æ•°åœ°é¢„æµ‹ã€‚

| æ€§èƒ½å‚æ•° |                                                              |
| -------- | ------------------------------------------------------------ |
| ***Nu*** | <img src="C:\Users\zt\AppData\Roaming\Typora\typora-user-images\image-20231014083121521.png" style="zoom:25%;" /> |
| ***f***  | <img src="C:\Users\zt\AppData\Roaming\Typora\typora-user-images\image-20231014083147236.png" style="zoom:25%;" /> |

â€‹		ä¸ºäº†è®¨è®ºä¸åŒæ ·æœ¬æ•°é‡å¯¹é¢„æµ‹æ•ˆæœçš„å½±å“ï¼Œæˆ‘ä»¬ä»»é€‰ä¸¤ç§é¢„æµ‹æ•ˆæœè¾ƒå¥½çš„ç¥ç»ç®—å­ç½‘ç»œFNOå’Œæ•ˆæœè¾ƒå·®çš„U-Netï¼Œåˆ†åˆ«é€‰å–æ€»æ ·æœ¬æ•°é‡çš„60%ã€40%ã€20%ã€10%ã€5%å’Œ2.5%ï¼Œå¯¹é¢„æµ‹æ•ˆæœè¿›è¡Œå¯¹æ¯”ã€‚ç»¼åˆFNOå’ŒU-Netåœ¨ä¸åŒæ ·æœ¬é‡ä¸‹ç‰©ç†åœºçš„å¹³å‡ç»å¯¹é¢„æµ‹è¯¯å·®ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼ŒFNOåœ¨è¾ƒå°‘æ ·æœ¬é‡ä¸‹å®ç°äº†è¾ƒé«˜ç²¾åº¦çš„ç‰©ç†åœºé¢„æµ‹ã€‚ä¾‹å¦‚åªé€‰å–5%çš„æ ·æœ¬å³å¯è¾¾åˆ°å°äº0.1çš„ç‰©ç†åœºè¯¯å·®ï¼Œé€‰å–40%çš„æ ·æœ¬å¯ä»¥è·å¾—æœ€ä½³çš„é¢„æµ‹æ•ˆæœã€‚è€ŒU-Netéœ€è¦40%çš„æ ·æœ¬æ‰å¯è¾¾åˆ°å°äº0.1çš„è¯¯å·®ï¼Œ60%çš„æ ·æœ¬è¾¾åˆ°æœ€ä¼˜çš„é¢„æµ‹æ•ˆæœã€‚å› æ­¤ï¼Œåœ¨è®­ç»ƒæ ·æœ¬æ•°é‡çš„è§’åº¦æ¥çœ‹ï¼ŒFNOçš„è®¡ç®—æ‰€éœ€èµ„æºè¿œå°äºU-Netã€‚

| è®­ç»ƒé›†å¤§å°å¯¹ç‰©ç†åœºå¹³å‡ç›¸å¯¹è¯¯å·®çš„å½±å“ | FNO                                                          | U-Net                                                        |
| ------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **FMAEAD**                           | <img src="C:\Users\zt\AppData\Roaming\Typora\typora-user-images\image-20231014083623926.png" style="zoom:25%;" /> | <img src="C:\Users\zt\AppData\Roaming\Typora\typora-user-images\image-20231014083721214.png" style="zoom:25%;" /> |

â€‹		æœ€åï¼Œæˆ‘ä»¬åœ¨è®¡ç®—æˆæœ¬æ–¹é¢å¯¹äº”ç§ç¥ç»ç®—å­ç½‘ç»œæ¨¡å‹è¿›è¡Œäº†æ›´è¯¦ç»†çš„æ¯”è¾ƒã€‚å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼ŒFNOå ç”¨æ˜¾å­˜æœ€å°‘ï¼ŒDeepONetå ç”¨æ˜¾å­˜æœ€å¤šï¼Œä¸”åˆ†åˆ«éœ€è¦å ç”¨1.21 GBå’Œ 6.43 GBçš„æ˜¾å­˜ï¼Œåè€…çš„éœ€æ±‚å¤§çº¦æ˜¯å‰è€…çš„5å€ã€‚åœ¨è®­ç»ƒæ—¶é—´æ–¹é¢ï¼ŒDeepONet è®­ç»ƒæ—¶é—´æœ€é•¿ï¼Œç›¸æ¯”æ—¶é—´æœ€çŸ­çš„FNOå¢åŠ äº†3 å€ã€‚æ­¤å¤–ï¼ŒTransformerå’ŒU-Netçš„æ€»å‚æ•°é‡æœ€å¤šï¼Œå¤§é‡æ•°æ®éœ€æ±‚æ˜¯ç”±äºå…¶å¤§é‡çš„å†…å­˜æˆæœ¬ï¼Œè¾ƒé•¿çš„è®­ç»ƒæ—¶é—´ï¼Œä»¥åŠéœ€è¦å¤§é‡çš„è®­ç»ƒæ ·æœ¬æ¥å®ç°å¯¹ç‰©ç†åœºçš„ç²¾ç¡®è¯†åˆ«ã€‚ä½†ç»¼åˆç‰©ç†åœºåŠæŸå¤±ç­‰å¯¹æ¯”ï¼ŒTransformerå’ŒFNOåœ¨é¢„æµ‹ç²¾åº¦å’Œè®­ç»ƒæˆæœ¬ä¸Šæœ€ä¼˜ï¼ŒU-Netå’ŒDeepONetçš„é¢„æµ‹ç²¾åº¦å’Œè®­ç»ƒæˆæœ¬æœ€å·®ã€‚ä¸”é¢„æµ‹æ•ˆæœæœ€ä¼˜çš„Transformeræ¯”é¢„æµ‹æ•ˆæœæœ€å·®çš„DeepONetçš„é¢„æµ‹ç²¾åº¦æé«˜äº†1ä¸ªæ•°é‡çº§ã€‚

| è®­ç»ƒæˆæœ¬            | FNO       | U-Net       | FNN     | DeepONet | Transformer |
| ------------------- | --------- | ----------- | ------- | -------- | ----------- |
| æ­¥é•¿                | 400       | 500         | 800     | 800      | 800         |
| è®­ç»ƒæ ·æœ¬æ•°/æ€»æ ·æœ¬æ•° | 0.6       | 0.6         | 0.6     | 0.6      | 0.6         |
| æ€»å‚æ•°é‡            | 2,106,468 | 212,488,324 | 120,263 | 6,8032   | 21,117,220  |
| æ¨¡å‹å¤§å° (MB)       | 0.04      | 547.99      | 0.28    | 0.19     | 54.11       |
| æ˜¾å­˜å ç”¨ (GB)       | 1.21      | 2.17        | 5.62    | 6.43     | 2.48        |
| è®­ç»ƒæ—¶é—´ (h)        | 1.02      | 1.81        | 3.11    | 3.25     | 1.70        |

â€‹		**æœ¬æ¬¡ä»£ç å’Œæµ‹è¯•ç»“æœä»…ä»…æŠ›ç –å¼•ç‰ï¼ŒæœŸå¾…æœ‰å…´è¶£çš„ä½¿ç”¨è€…åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œè¿›ä¸€æ­¥æ¢ç´¢ã€‚**



## 6.æ¨¡å‹ä¿¡æ¯

| ä¿¡æ¯     | è¯´æ˜                              |
| -------- | --------------------------------- |
| å‘å¸ƒè€…   | tianshao1992                      |
| æ—¶é—´     | 2023.10                           |
| æ¡†æ¶ç‰ˆæœ¬ | Paddle Develope                   |
| åº”ç”¨åœºæ™¯ | ç§‘å­¦è®¡ç®—                          |
| æ”¯æŒç¡¬ä»¶ | AMD EPYC 7642 CPUã€Nvidia A40 GPU |



## Reference

[1]   LIU T Y, LI Y Z, XIE Y H, et al. Deep Learning for Nanofluid Field Reconstruction in Experimental Analysis [J]. IEEE Access, 2020, 8: 64692-706.

[2]  LIU T Y, LI Y Z, JING Q, et al. Supervised learning method for the physical field reconstruction in a nanofluid heat transfer problem [J]. International Journal of Heat and Mass Transfer, 2021, 165: 24.

[3]  Li Z Y,  Zheng N, NIKOLA Kovachki, et al., Physics-Informed Neural Operator for Learning  Partial Differential Equations [J], arXiv: 2111.03794, 2021. 

[4] Ronneberger O, Fischer P, Brox T. U-Net: Convolutional Networks for Biomedical Image Segmentation. In: Navab N, Hornegger J, Wells WM, Frangi AF, editors. Med. Image Comput. Comput. Interv. â€“ MICCAI 2015, Cham: Springer International Publishing; 2015, p. 234â€“41.

[5] HAGHIGHAT E, RAISSI M, MOURE A, et al. A physics-informed deep learning framework for inversion and surrogate modeling in solid mechanics [J]. Comput Meth Appl Mech Eng, 2021, 379: 22.

[6] LU L, JIN P Z, PANG G F, et al. Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators [J]. Nat Mach Intell, 2021, 3(3): 218.

[7] CAO S H. Choose a Transformer: Fourier or Galerkin; proceedings of the Neural Information Processing Systems, F, 2021 [J]. arXiv: 2105.14995, 2021.

[8] LI Y Z, LIU T Y, XIE Y H. Thermal fluid fields reconstruction for nanofluids convection based on physics-informed deep learning [J]. Sci Rep, 2022, 12(1): 23.

